{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Geospatial tools","text":""},{"location":"#description","title":"Description","text":"<p>This repository is a collection of tools and scripts for geospatial use cases.</p> <p>For more detailed information on how to install, configure and develop a project using this repository, please refer yourself to the project's README</p>"},{"location":"#requirements","title":"Requirements","text":"<p>This project has only been tested in a Linux (Debian based) environment and assumes some basic tools for development are already installed.</p> <p>The project uses a Makefile to automate most operations. If <code>make</code> is available on your machine there's a good chance this will work.</p>"},{"location":"#python-version","title":"Python Version","text":"<p>This project uses Python version 3.11</p>"},{"location":"#build-tool","title":"Build Tool","text":"<p>This project uses <code>poetry</code> as a build tool. Using a build tool has the advantage of streamlining script use as well as fix path issues related to imports.</p>"},{"location":"reference/SUMMARY/","title":"SUMMARY","text":"<ul> <li>download</li> <li>geotools_types</li> <li>planetary_computer<ul> <li>sentinel_2</li> </ul> </li> <li>radar<ul> <li>nimrod</li> </ul> </li> <li>raster</li> <li>stac</li> <li>utils</li> <li>vector</li> </ul>"},{"location":"reference/download/","title":"download","text":""},{"location":"reference/download/#download.download_usa_polygon","title":"download_usa_polygon","text":"<pre><code>download_usa_polygon(\n    output_name: str = USA_POLYGON, output_directory: str | Path = DATA_DIR\n) -&gt; list[str | Path]\n</code></pre> <p>Download USA polygon file.</p> <p>Parameters:</p> Name Type Description Default <code>output_name</code> <code>str</code> <p>What name to give to downloaded file</p> <code>USA_POLYGON</code> <code>output_directory</code> <code>str | Path</code> <p>Where to save the downloaded file</p> <code>DATA_DIR</code> <p>Returns:</p> Source code in <code>geospatial_tools/download.py</code> <pre><code>def download_usa_polygon(output_name: str = USA_POLYGON, output_directory: str | Path = DATA_DIR) -&gt; list[str | Path]:\n    \"\"\"\n    Download USA polygon file.\n\n    Args:\n      output_name: What name to give to downloaded file\n      output_directory: Where to save the downloaded file\n\n    Returns:\n    \"\"\"\n    file_list = _download_from_link(\n        target_download=USA_POLYGON, output_name=output_name, output_directory=output_directory\n    )\n    return file_list\n</code></pre>"},{"location":"reference/download/#download.download_s2_tiling_grid","title":"download_s2_tiling_grid","text":"<pre><code>download_s2_tiling_grid(\n    output_name: str = SENTINEL_2_TILLING_GRID, output_directory: str | Path = DATA_DIR\n) -&gt; list[str | Path]\n</code></pre> <p>\" Download Sentinel 2 tiling grid file.</p> <p>Parameters:</p> Name Type Description Default <code>output_name</code> <code>str</code> <p>What name to give to downloaded file</p> <code>SENTINEL_2_TILLING_GRID</code> <code>output_directory</code> <code>str | Path</code> <p>Where to save the downloaded file</p> <code>DATA_DIR</code> <p>Returns:</p> Source code in <code>geospatial_tools/download.py</code> <pre><code>def download_s2_tiling_grid(\n    output_name: str = SENTINEL_2_TILLING_GRID, output_directory: str | Path = DATA_DIR\n) -&gt; list[str | Path]:\n    \"\"\"\n    \" Download Sentinel 2 tiling grid file.\n\n    Args:\n      output_name: What name to give to downloaded file\n      output_directory: Where to save the downloaded file\n\n    Returns:\n    \"\"\"\n    file_list = _download_from_link(\n        target_download=SENTINEL_2_TILLING_GRID, output_name=output_name, output_directory=output_directory\n    )\n    return file_list\n</code></pre>"},{"location":"reference/geotools_types/","title":"geotools_types","text":"<p>This module contains constants and functions pertaining to data types.</p>"},{"location":"reference/geotools_types/#geotools_types.BBoxLike","title":"BBoxLike  <code>module-attribute</code>","text":"<pre><code>BBoxLike = tuple[float, float, float, float]\n</code></pre> <p>BBox like tuple structure used for type checking.</p>"},{"location":"reference/geotools_types/#geotools_types.IntersectsLike","title":"IntersectsLike  <code>module-attribute</code>","text":"<pre><code>IntersectsLike = Union[\n    Point,\n    Polygon,\n    LineString,\n    MultiPolygon,\n    MultiPoint,\n    MultiLineString,\n    GeometryCollection,\n]\n</code></pre> <p>Intersect-like union of types used for type checking.</p>"},{"location":"reference/geotools_types/#geotools_types.DateLike","title":"DateLike  <code>module-attribute</code>","text":"<pre><code>DateLike = Union[\n    datetime,\n    str,\n    None,\n    tuple[Union[datetime, str, None], Union[datetime, str, None]],\n    list[Union[datetime, str, None]],\n    Iterator[Union[datetime, str, None]],\n]\n</code></pre> <p>Date-like union of types used for type checking.</p>"},{"location":"reference/raster/","title":"raster","text":"<p>This module contains functions that process or create raster/image data.</p>"},{"location":"reference/raster/#raster.reproject_raster","title":"reproject_raster","text":"<pre><code>reproject_raster(\n    dataset_path: str | Path,\n    target_crs: str | int,\n    target_path: str | Path,\n    logger: Logger = LOGGER,\n) -&gt; Path | None\n</code></pre> <p>Parameters:</p> Name Type Description Default <code>dataset_path</code> <code>str | Path</code> <p>Path to the dataset to be reprojected.</p> required <code>target_crs</code> <code>str | int</code> <p>EPSG code in string or int format. Can be given in the following ways: 5070 | \"5070\" | \"EPSG:5070\"</p> required <code>target_path</code> <code>str | Path</code> <p>Path and filename for reprojected dataset.</p> required <code>logger</code> <code>Logger</code> <p>Logger instance</p> <code>LOGGER</code> <p>Returns:</p> Source code in <code>geospatial_tools/raster.py</code> <pre><code>def reproject_raster(\n    dataset_path: str | pathlib.Path,\n    target_crs: str | int,\n    target_path: str | pathlib.Path,\n    logger: logging.Logger = LOGGER,\n) -&gt; pathlib.Path | None:\n    \"\"\"\n\n    Args:\n      dataset_path: Path to the dataset to be reprojected.\n      target_crs: EPSG code in string or int format. Can be given in the following ways: 5070 | \"5070\" | \"EPSG:5070\"\n      target_path: Path and filename for reprojected dataset.\n      logger: Logger instance\n\n    Returns:\n\n\n    \"\"\"\n    if isinstance(dataset_path, str):\n        dataset_path = pathlib.Path(dataset_path)\n\n    if isinstance(target_path, str):\n        target_path = pathlib.Path(target_path)\n\n    target_crs = create_crs(target_crs, logger=logger)\n\n    with rasterio.open(dataset_path) as source_dataset:\n        transform, width, height = calculate_default_transform(\n            source_dataset.crs, target_crs, source_dataset.width, source_dataset.height, *source_dataset.bounds\n        )\n        kwargs = source_dataset.meta.copy()\n        kwargs.update({\"crs\": target_crs, \"transform\": transform, \"width\": width, \"height\": height})\n\n        with rasterio.open(target_path, \"w\", **kwargs) as reprojected_dataset:\n            for i in range(1, source_dataset.count + 1):\n                reproject(\n                    source=rasterio.band(source_dataset, i),\n                    destination=rasterio.band(reprojected_dataset, i),\n                    src_transform=source_dataset.transform,\n                    src_crs=source_dataset.crs,\n                    dst_transform=transform,\n                    dst_crs=target_crs,\n                    resampling=Resampling.nearest,\n                )\n    if target_path.exists():\n        logger.info(f\"Reprojected file created at {target_path}\")\n        return target_path\n    logger.error(f\"Failed to reproject file {dataset_path}\")\n    return None\n</code></pre>"},{"location":"reference/raster/#raster.clip_raster_with_polygon","title":"clip_raster_with_polygon","text":"<pre><code>clip_raster_with_polygon(\n    raster_image: Path | str,\n    polygon_layer: Path | str | GeoDataFrame,\n    base_output_filename: str | None = None,\n    output_dir: str | Path = DATA_DIR,\n    num_of_workers: int | None = None,\n    logger: Logger = LOGGER,\n) -&gt; list[Path]\n</code></pre> <p>Parameters:</p> Name Type Description Default <code>raster_image</code> <code>Path | str</code> <p>Path to raster image to be clipped.</p> required <code>polygon_layer</code> <code>Path | str | GeoDataFrame</code> <p>Polygon layer which polygons will be used to clip the raster image.</p> required <code>base_output_filename</code> <code>str | None</code> <p>Base filename for outputs. If <code>None</code>, will be taken from input polygon layer.</p> <code>None</code> <code>output_dir</code> <code>str | Path</code> <p>Directory path where output will be written.</p> <code>DATA_DIR</code> <code>num_of_workers</code> <code>int | None</code> <p>The number of processes to use for parallel execution. If using on a compute cluster, please set a specific amount (ex. 1 per CPU core requested). Defaults to <code>cpu_count()</code>.</p> <code>None</code> <code>logger</code> <code>Logger</code> <p>Logger instance</p> <code>LOGGER</code> <p>Returns:</p> Source code in <code>geospatial_tools/raster.py</code> <pre><code>def clip_raster_with_polygon(\n    raster_image: pathlib.Path | str,\n    polygon_layer: pathlib.Path | str | GeoDataFrame,\n    base_output_filename: str | None = None,\n    output_dir: str | pathlib.Path = DATA_DIR,\n    num_of_workers: int | None = None,\n    logger: logging.Logger = LOGGER,\n) -&gt; list[pathlib.Path]:\n    \"\"\"\n\n    Args:\n      raster_image: Path to raster image to be clipped.\n      polygon_layer: Polygon layer which polygons will be used to clip the raster image.\n      base_output_filename: Base filename for outputs. If `None`, will be taken from\n        input polygon layer.\n      output_dir: Directory path where output will be written.\n      num_of_workers: The number of processes to use for parallel execution. If using\n        on a compute cluster, please set a specific amount (ex. 1 per CPU core requested).\n        Defaults to `cpu_count()`.\n      logger: Logger instance\n\n    Returns:\n\n\n    \"\"\"\n    workers = cpu_count()\n    if num_of_workers:\n        workers = num_of_workers\n\n    logger.info(f\"Number of workers used: {workers}\")\n    logger.info(f\"Output path : [{output_dir}]\")\n\n    if isinstance(raster_image, str):\n        raster_image = pathlib.Path(raster_image)\n    if not raster_image.exists():\n        logger.error(\"Raster image does not exist\")\n        return []\n\n    if not base_output_filename:\n        base_output_filename = raster_image.stem\n\n    if isinstance(output_dir, str):\n        output_dir = pathlib.Path(output_dir)\n    output_dir.mkdir(parents=True, exist_ok=True)\n\n    gdf = polygon_layer\n    if not isinstance(polygon_layer, GeoDataFrame):\n        gdf = gpd.read_file(polygon_layer)\n\n    polygons = gdf[\"geometry\"]\n    ids = gdf.index\n\n    id_polygon_list = zip(ids, polygons, strict=False)\n    logger.info(f\"Clipping raster image with {len(polygons)} polygons\")\n    with ProcessPoolExecutor(max_workers=workers) as executor:\n        futures = [\n            executor.submit(\n                _clip_process,\n                raster_image=raster_image,\n                id_polygon=polygon_tuple,\n                base_output_filename=base_output_filename,\n                output_dir=output_dir,\n                logger=logger,\n            )\n            for polygon_tuple in id_polygon_list\n        ]\n        results = [future.result() for future in concurrent.futures.as_completed(futures)]\n    path_list = []\n    for result in results:\n        if isinstance(result, tuple):\n            logger.debug(f\"Writing file successful : [{result}]\")\n            path_list.append(result[2])\n    logger.info(\"Clipping process finished\")\n    return path_list\n</code></pre>"},{"location":"reference/raster/#raster.get_total_band_count","title":"get_total_band_count","text":"<pre><code>get_total_band_count(\n    raster_file_list: list[Path | str], logger: Logger = LOGGER\n) -&gt; int\n</code></pre> <p>Parameters:</p> Name Type Description Default <code>raster_file_list</code> <code>list[Path | str]</code> <p>List of raster files to be processed.</p> required <code>logger</code> <code>Logger</code> <p>Logger instance</p> <code>LOGGER</code> <p>Returns:</p> Source code in <code>geospatial_tools/raster.py</code> <pre><code>def get_total_band_count(raster_file_list: list[pathlib.Path | str], logger: logging.Logger = LOGGER) -&gt; int:\n    \"\"\"\n\n    Args:\n      raster_file_list: List of raster files to be processed.\n      logger: Logger instance\n\n    Returns:\n\n\n    \"\"\"\n    total_band_count = 0\n    for raster in raster_file_list:\n        with rasterio.open(raster, \"r\") as raster_image:\n            total_band_count += raster_image.count\n    logger.info(f\"Calculated a total of [{total_band_count}] bands\")\n    return total_band_count\n</code></pre>"},{"location":"reference/raster/#raster.create_merged_raster_bands_metadata","title":"create_merged_raster_bands_metadata","text":"<pre><code>create_merged_raster_bands_metadata(\n    raster_file_list: list[Path | str], logger: Logger = LOGGER\n) -&gt; dict\n</code></pre> <p>Parameters:</p> Name Type Description Default <code>raster_file_list</code> <code>list[Path | str]</code> required <code>logger</code> <code>Logger</code> <code>LOGGER</code> <p>Returns:</p> Source code in <code>geospatial_tools/raster.py</code> <pre><code>def create_merged_raster_bands_metadata(\n    raster_file_list: list[pathlib.Path | str], logger: logging.Logger = LOGGER\n) -&gt; dict:\n    \"\"\"\n\n    Args:\n      raster_file_list:\n      logger:\n\n    Returns:\n\n\n    \"\"\"\n    logger.info(\"Creating merged asset metadata\")\n    total_band_count = get_total_band_count(raster_file_list)\n    with rasterio.open(raster_file_list[0]) as meta_source:\n        meta = meta_source.meta\n        meta.update(count=total_band_count)\n    return meta\n</code></pre>"},{"location":"reference/raster/#raster.merge_raster_bands","title":"merge_raster_bands","text":"<pre><code>merge_raster_bands(\n    raster_file_list: list[Path | str],\n    merged_filename: Path | str,\n    merged_band_names: list[str] = None,\n    merged_metadata: dict = None,\n    logger: Logger = LOGGER,\n) -&gt; Path | None\n</code></pre> <p>This function aims to combine multiple overlapping raster bands into a single raster image.</p> <p>Example use case: I have 3 bands, B0, B1 and B2, each as an independent raster file (like is the case with downloaded STAC data.</p> <p>While it can probably be used to create spatial time series, and not just combine bands from a single image product, it has not yet been tested for that specific purpose.</p> <p>Parameters:</p> Name Type Description Default <code>raster_file_list</code> <code>list[Path | str]</code> <p>List of raster files to be processed.</p> required <code>merged_filename</code> <code>Path | str</code> <p>Name of output raster file.</p> required <code>merged_metadata</code> <code>dict</code> <p>Dictionary of metadata to use if you prefer to great it independently.</p> <code>None</code> <code>merged_band_names</code> <code>list[str]</code> <p>Names of final output raster bands. For example : I have 3 images representing each a single band; raster_file_list =  [\"image01_B0.tif\", \"image01_B1.tif\", \"image01_B2.tif\"]. With, merged_band_names, individual band id can be assigned for the final output raster; [\"B0\", \"B1\", \"B2\"].</p> <code>None</code> <code>logger</code> <code>Logger</code> <p>Logger instance</p> <code>LOGGER</code> <p>Returns:</p> Type Description <code>Path | None</code> <p>Path to merged raster</p> Source code in <code>geospatial_tools/raster.py</code> <pre><code>def merge_raster_bands(\n    raster_file_list: list[pathlib.Path | str],\n    merged_filename: pathlib.Path | str,\n    merged_band_names: list[str] = None,\n    merged_metadata: dict = None,\n    logger: logging.Logger = LOGGER,\n) -&gt; pathlib.Path | None:\n    \"\"\"\n    This function aims to combine multiple overlapping raster bands into a single raster image.\n\n    Example use case: I have 3 bands, B0, B1 and B2, each as an independent raster file (like is the case with\n    downloaded STAC data.\n\n    While it can probably be used to create spatial time series, and not just combine bands\n    from a single image product, it has not yet been tested for that specific purpose.\n\n    Args:\n      raster_file_list: List of raster files to be processed.\n      merged_filename: Name of output raster file.\n      merged_metadata: Dictionary of metadata to use if you prefer to great it independently.\n      merged_band_names: Names of final output raster bands. For example : I have 3 images representing each\n        a single band; raster_file_list =  [\"image01_B0.tif\", \"image01_B1.tif\", \"image01_B2.tif\"].\n        With, merged_band_names, individual band id can be assigned for the final output raster;\n        [\"B0\", \"B1\", \"B2\"].\n      logger: Logger instance\n\n    Returns:\n        Path to merged raster\n    \"\"\"\n    if not merged_metadata:\n        merged_metadata = create_merged_raster_bands_metadata(raster_file_list)\n\n    merged_image_index = 1\n    band_names_index = 0\n\n    logger.info(f\"Merging asset [{merged_filename}] ...\")\n    # Create the final raster image in which all bands will be written to\n    with rasterio.open(merged_filename, \"w\", **merged_metadata) as merged_asset_image:\n        # Iterate through the raster file list to be merged\n        for raster_file in raster_file_list:\n            asset_name = pathlib.Path(raster_file).name\n            logger.info(f\"Writing band image: {asset_name}\")\n            with rasterio.open(raster_file) as source_image:\n                num_of_bands = source_image.count\n\n                # Iterate through each band of the raster file\n                for source_image_band_index in range(1, num_of_bands + 1):\n                    logger.info(\n                        f\"Writing asset sub item band {source_image_band_index} \"\n                        f\"to merged index band {merged_image_index}\"\n                    )\n                    # Write band to output merged_asset_image\n                    merged_asset_image.write_band(merged_image_index, source_image.read(source_image_band_index))\n                    _handle_band_metadata(\n                        source_image=source_image,\n                        source_image_band_index=source_image_band_index,\n                        band_names_index=band_names_index,\n                        merged_asset_image=merged_asset_image,\n                        merged_band_names=merged_band_names,\n                        merged_image_index=merged_image_index,\n                    )\n                    merged_image_index += 1\n                band_names_index += 1\n\n    if not merged_filename.exists():\n        return None\n\n    return merged_filename\n</code></pre>"},{"location":"reference/stac/","title":"stac","text":"<p>This module contains functions that are related to STAC API.</p>"},{"location":"reference/stac/#stac.AssetSubItem","title":"AssetSubItem","text":"<pre><code>AssetSubItem(asset, item_id: str, band: str, filename: str | Path)\n</code></pre> <p>Class that represent a STAC asset sub item.</p> <p>Generally represents a single satellite image band.</p> <p>Parameters:</p> Name Type Description Default <code>asset</code> required <code>item_id</code> <code>str</code> required <code>band</code> <code>str</code> required <code>filename</code> <code>str | Path</code> required Source code in <code>geospatial_tools/stac.py</code> <pre><code>def __init__(self, asset, item_id: str, band: str, filename: str | Path):\n    \"\"\"\n\n    Args:\n        asset:\n        item_id:\n        band:\n        filename:\n    \"\"\"\n    if isinstance(filename, str):\n        filename = Path(filename)\n    self.asset = asset\n    self.item_id: str = item_id\n    self.band: str = band\n    self.filename: Path = filename\n</code></pre>"},{"location":"reference/stac/#stac.Asset","title":"Asset","text":"<pre><code>Asset(\n    asset_id: str,\n    bands: list[str] | None = None,\n    asset_item_list: list[AssetSubItem] | None = None,\n    merged_asset_path: str | Path | None = None,\n    reprojected_asset: str | Path | None = None,\n    logger: Logger = LOGGER,\n)\n</code></pre> <p>Represents a STAC asset.</p> <p>Parameters:</p> Name Type Description Default <code>asset_id</code> <code>str</code> required <code>bands</code> <code>list[str] | None</code> <code>None</code> <code>asset_item_list</code> <code>list[AssetSubItem] | None</code> <code>None</code> <code>merged_asset_path</code> <code>str | Path | None</code> <code>None</code> <code>reprojected_asset</code> <code>str | Path | None</code> <code>None</code> <code>logger</code> <code>Logger</code> <code>LOGGER</code> Source code in <code>geospatial_tools/stac.py</code> <pre><code>def __init__(\n    self,\n    asset_id: str,\n    bands: list[str] | None = None,\n    asset_item_list: list[AssetSubItem] | None = None,\n    merged_asset_path: str | Path | None = None,\n    reprojected_asset: str | Path | None = None,\n    logger: logging.Logger = LOGGER,\n):\n    \"\"\"\n\n    Args:\n        asset_id:\n        bands:\n        asset_item_list:\n        merged_asset_path:\n        reprojected_asset:\n        logger:\n    \"\"\"\n    self.asset_id = asset_id\n    self.bands = bands\n    self.list = asset_item_list\n    self.merged_asset_path = merged_asset_path\n    self.reprojected_asset_path = reprojected_asset\n    self.logger = logger\n</code></pre>"},{"location":"reference/stac/#stac.Asset.add_asset_item","title":"add_asset_item","text":"<pre><code>add_asset_item(asset: AssetSubItem)\n</code></pre> <p>Parameters:</p> Name Type Description Default <code>asset</code> <code>AssetSubItem</code> <p>AssetSubItem:</p> required <p>Returns:</p> Source code in <code>geospatial_tools/stac.py</code> <pre><code>def add_asset_item(self, asset: AssetSubItem):\n    \"\"\"\n\n    Args:\n      asset: AssetSubItem:\n\n    Returns:\n\n\n    \"\"\"\n    if not self.list:\n        self.list = []\n    self.list.append(asset)\n</code></pre>"},{"location":"reference/stac/#stac.Asset.show_asset_items","title":"show_asset_items","text":"<pre><code>show_asset_items()\n</code></pre> <p>Show items that belong to this asset.</p> Source code in <code>geospatial_tools/stac.py</code> <pre><code>def show_asset_items(self):\n    \"\"\"Show items that belong to this asset.\"\"\"\n    asset_list = []\n    for asset_sub_item in self.list:\n        asset_list.append(\n            f\"ID: [{asset_sub_item.item_id}], Band: [{asset_sub_item.band}], filename: [{asset_sub_item.filename}]\"\n        )\n    self.logger.info(f\"Asset list for asset [{self.asset_id}] : \\n\\t{asset_list}\")\n</code></pre>"},{"location":"reference/stac/#stac.Asset.merge_asset","title":"merge_asset","text":"<pre><code>merge_asset(\n    base_directory: str | Path | None = None, delete_sub_items: bool = False\n) -&gt; Path | None\n</code></pre> <p>Parameters:</p> Name Type Description Default <code>base_directory</code> <code>str | Path | None</code> <code>None</code> <code>delete_sub_items</code> <code>bool</code> <code>False</code> <p>Returns:</p> Source code in <code>geospatial_tools/stac.py</code> <pre><code>def merge_asset(self, base_directory: str | Path | None = None, delete_sub_items: bool = False) -&gt; Path | None:\n    \"\"\"\n\n    Args:\n      base_directory:\n      delete_sub_items:\n\n    Returns:\n\n\n    \"\"\"\n    if not base_directory:\n        base_directory = \"\"\n    if isinstance(base_directory, str):\n        base_directory = Path(base_directory)\n\n    merged_filename = base_directory / f\"{self.asset_id}_merged.tif\"\n\n    asset_filename_list = [asset.filename for asset in self.list]\n\n    meta = self._create_merged_asset_metadata()\n\n    merge_raster_bands(\n        merged_filename=merged_filename,\n        raster_file_list=asset_filename_list,\n        merged_metadata=meta,\n        merged_band_names=self.bands,\n    )\n\n    if merged_filename.exists():\n        self.logger.info(f\"Asset [{self.asset_id}] merged successfully\")\n        self.logger.info(f\"Asset location : [{merged_filename}]\")\n        self.merged_asset_path = merged_filename\n        if delete_sub_items:\n            self.delete_asset_sub_items()\n        return merged_filename\n    self.logger.error(f\"There was a problem merging asset [{self.asset_id}]\")\n    return None\n</code></pre>"},{"location":"reference/stac/#stac.Asset.reproject_merged_asset","title":"reproject_merged_asset","text":"<pre><code>reproject_merged_asset(\n    target_projection: str | int,\n    base_directory: str | Path = None,\n    delete_merged_asset: bool = False,\n)\n</code></pre> <p>Parameters:</p> Name Type Description Default <code>target_projection</code> <code>str | int</code> required <code>base_directory</code> <code>str | Path</code> <code>None</code> <code>delete_merged_asset</code> <code>bool</code> <code>False</code> <p>Returns:</p> Source code in <code>geospatial_tools/stac.py</code> <pre><code>def reproject_merged_asset(\n    self,\n    target_projection: str | int,\n    base_directory: str | Path = None,\n    delete_merged_asset: bool = False,\n):\n    \"\"\"\n\n    Args:\n      target_projection:\n      base_directory:\n      delete_merged_asset:\n\n    Returns:\n\n\n    \"\"\"\n    if not base_directory:\n        base_directory = \"\"\n    if isinstance(base_directory, str):\n        base_directory = Path(base_directory)\n    target_path = base_directory / f\"{self.asset_id}_reprojected.tif\"\n    self.logger.info(f\"Reprojecting asset [{self.asset_id}] ...\")\n    reprojected_filename = reproject_raster(\n        dataset_path=self.merged_asset_path,\n        target_path=target_path,\n        target_crs=target_projection,\n        logger=self.logger,\n    )\n    if reprojected_filename.exists():\n        self.logger.info(f\"Asset location : [{reprojected_filename}]\")\n        self.reprojected_asset_path = reprojected_filename\n        if delete_merged_asset:\n            self.delete_merged_asset()\n        return reprojected_filename\n    self.logger.error(f\"There was a problem reprojecting asset [{self.asset_id}]\")\n    return None\n</code></pre>"},{"location":"reference/stac/#stac.Asset.delete_asset_sub_items","title":"delete_asset_sub_items","text":"<pre><code>delete_asset_sub_items()\n</code></pre> <p>Delete all asset sub items that belong to this asset.</p> Source code in <code>geospatial_tools/stac.py</code> <pre><code>def delete_asset_sub_items(self):\n    \"\"\"Delete all asset sub items that belong to this asset.\"\"\"\n    self.logger.info(f\"Deleting asset sub items from asset [{self.asset_id}]\")\n    if self.list:\n        for item in self.list:\n            self.logger.info(f\"Deleting [{item.filename}] ...\")\n            item.filename.unlink()\n</code></pre>"},{"location":"reference/stac/#stac.Asset.delete_merged_asset","title":"delete_merged_asset","text":"<pre><code>delete_merged_asset()\n</code></pre> <p>Delete merged asset.</p> Source code in <code>geospatial_tools/stac.py</code> <pre><code>def delete_merged_asset(self):\n    \"\"\"Delete merged asset.\"\"\"\n    self.logger.info(f\"Deleting merged asset file for [{self.merged_asset_path}]\")\n    self.merged_asset_path.unlink()\n</code></pre>"},{"location":"reference/stac/#stac.Asset.delete_reprojected_asset","title":"delete_reprojected_asset","text":"<pre><code>delete_reprojected_asset()\n</code></pre> <p>Delete reprojected asset.</p> Source code in <code>geospatial_tools/stac.py</code> <pre><code>def delete_reprojected_asset(self):\n    \"\"\"Delete reprojected asset.\"\"\"\n    self.logger.info(f\"Deleting reprojected asset file for [{self.reprojected_asset_path}]\")\n    self.reprojected_asset_path.unlink()\n</code></pre>"},{"location":"reference/stac/#stac.StacSearch","title":"StacSearch","text":"<pre><code>StacSearch(catalog_name, logger=LOGGER)\n</code></pre> <p>Utility class to help facilitate and automate STAC API searches through the use of <code>pystac_client.Client</code>.</p> <p>Parameters:</p> Name Type Description Default <code>catalog_name</code> required <code>logger</code> <code>LOGGER</code> Source code in <code>geospatial_tools/stac.py</code> <pre><code>def __init__(self, catalog_name, logger=LOGGER):\n    \"\"\"\n\n    Args:\n        catalog_name:\n        logger:\n    \"\"\"\n    self.catalog: pystac_client.Client = catalog_generator(catalog_name=catalog_name)\n    self.search_results: list[pystac.Item] | None = None\n    self.cloud_cover_sorted_results: list[pystac.Item] | None = None\n    self.filtered_results: list[pystac.Item] | None = None\n    self.downloaded_search_assets: list[Asset] | None = None\n    self.downloaded_cloud_cover_sorted_assets: list[Asset] | None = None\n    self.downloaded_best_sorted_asset = None\n    self.logger = logger\n</code></pre>"},{"location":"reference/stac/#stac.StacSearch.search","title":"search","text":"<pre><code>search(\n    date_range: DateLike = None,\n    max_items: int | None = None,\n    limit: int | None = None,\n    ids: list | None = None,\n    collections: str | list | None = None,\n    bbox: BBoxLike | None = None,\n    intersects: IntersectsLike | None = None,\n    query: dict | None = None,\n    sortby: list | dict | None = None,\n    max_retries: int = 3,\n    delay=5,\n) -&gt; list\n</code></pre> <p>STAC API search that will use search query and parameters. Essentially a wrapper on <code>pystac_client.Client</code>.</p> <p>Parameter descriptions taken from pystac docs.</p> <p>Parameters:</p> Name Type Description Default <code>date_range</code> <code>DateLike</code> <p>Either a single datetime or datetime range used to filter results.     You may express a single datetime using a :class:<code>datetime.datetime</code>     instance, a <code>RFC 3339-compliant &lt;https://tools.ietf.org/html/rfc3339&gt;</code>__     timestamp, or a simple date string (see below). Instances of     :class:<code>datetime.datetime</code> may be either     timezone aware or unaware. Timezone aware instances will be converted to     a UTC timestamp before being passed     to the endpoint. Timezone unaware instances are assumed to represent UTC     timestamps. You may represent a     datetime range using a <code>\"/\"</code> separated string as described in the     spec, or a list, tuple, or iterator     of 2 timestamps or datetime instances. For open-ended ranges, use either     <code>\"..\"</code> (<code>'2020-01-01:00:00:00Z/..'</code>,     <code>['2020-01-01:00:00:00Z', '..']</code>) or a value of <code>None</code>     (<code>['2020-01-01:00:00:00Z', None]</code>).     If using a simple date string, the datetime can be specified in     <code>YYYY-mm-dd</code> format, optionally truncating     to <code>YYYY-mm</code> or just <code>YYYY</code>. Simple date strings will be expanded to     include the entire time period, for example: <code>2017</code> expands to     <code>2017-01-01T00:00:00Z/2017-12-31T23:59:59Z</code> and <code>2017-06</code> expands     to <code>2017-06-01T00:00:00Z/2017-06-30T23:59:59Z</code>     If used in a range, the end of the range expands to the end of that     day/month/year, for example: <code>2017-06-10/2017-06-11</code> expands to       <code>2017-06-10T00:00:00Z/2017-06-11T23:59:59Z</code> (Default value = None)</p> <code>None</code> <code>max_items</code> <code>int | None</code> <p>The maximum number of items to return from the search, even if there are more matching results.</p> <code>None</code> <code>limit</code> <code>int | None</code> <p>A recommendation to the service as to the number of items to return per page of results.</p> <code>None</code> <code>ids</code> <code>list | None</code> <p>List of one or more Item ids to filter on.</p> <code>None</code> <code>collections</code> <code>str | list | None</code> <p>List of one or more Collection IDs or pystac. Collection instances. Only Items in one of the provided Collections will be searched</p> <code>None</code> <code>bbox</code> <code>BBoxLike | None</code> <p>A list, tuple, or iterator representing a bounding box of 2D or 3D coordinates. Results will be filtered to only those intersecting the bounding box.</p> <code>None</code> <code>intersects</code> <code>IntersectsLike | None</code> <p>A string or dictionary representing a GeoJSON geometry, or an object that implements a geo_interface property, as supported by several libraries including Shapely, ArcPy, PySAL, and geojson. Results filtered to only those intersecting the geometry.</p> <code>None</code> <code>query</code> <code>dict | None</code> <p>List or JSON of query parameters as per the STAC API query extension.</p> <code>None</code> <code>sortby</code> <code>list | dict | None</code> <p>A single field or list of fields to sort the response by</p> <code>None</code> <code>max_retries</code> <code>int</code> <code>3</code> <code>delay</code> <code>5</code> <p>Returns:</p> Source code in <code>geospatial_tools/stac.py</code> <pre><code>def search(\n    self,\n    date_range: DateLike = None,\n    max_items: int | None = None,\n    limit: int | None = None,\n    ids: list | None = None,\n    collections: str | list | None = None,\n    bbox: geotools_types.BBoxLike | None = None,\n    intersects: geotools_types.IntersectsLike | None = None,\n    query: dict | None = None,\n    sortby: list | dict | None = None,\n    max_retries: int = 3,\n    delay=5,\n) -&gt; list:\n    \"\"\"\n    STAC API search that will use search query and parameters. Essentially a wrapper on `pystac_client.Client`.\n\n    Parameter descriptions taken from pystac docs.\n\n    Args:\n      date_range: Either a single datetime or datetime range used to filter results.\n            You may express a single datetime using a :class:`datetime.datetime`\n            instance, a `RFC 3339-compliant &lt;https://tools.ietf.org/html/rfc3339&gt;`__\n            timestamp, or a simple date string (see below). Instances of\n            :class:`datetime.datetime` may be either\n            timezone aware or unaware. Timezone aware instances will be converted to\n            a UTC timestamp before being passed\n            to the endpoint. Timezone unaware instances are assumed to represent UTC\n            timestamps. You may represent a\n            datetime range using a ``\"/\"`` separated string as described in the\n            spec, or a list, tuple, or iterator\n            of 2 timestamps or datetime instances. For open-ended ranges, use either\n            ``\"..\"`` (``'2020-01-01:00:00:00Z/..'``,\n            ``['2020-01-01:00:00:00Z', '..']``) or a value of ``None``\n            (``['2020-01-01:00:00:00Z', None]``).\n            If using a simple date string, the datetime can be specified in\n            ``YYYY-mm-dd`` format, optionally truncating\n            to ``YYYY-mm`` or just ``YYYY``. Simple date strings will be expanded to\n            include the entire time period, for example: ``2017`` expands to\n            ``2017-01-01T00:00:00Z/2017-12-31T23:59:59Z`` and ``2017-06`` expands\n            to ``2017-06-01T00:00:00Z/2017-06-30T23:59:59Z``\n            If used in a range, the end of the range expands to the end of that\n            day/month/year, for example: ``2017-06-10/2017-06-11`` expands to\n              ``2017-06-10T00:00:00Z/2017-06-11T23:59:59Z`` (Default value = None)\n      max_items: The maximum number of items to return from the search, even if there are\n        more matching results.\n      limit: A recommendation to the service as to the number of items to return per\n        page of results.\n      ids: List of one or more Item ids to filter on.\n      collections: List of one or more Collection IDs or pystac. Collection instances. Only Items in one of the\n        provided Collections will be searched\n      bbox: A list, tuple, or iterator representing a bounding box of 2D or 3D coordinates. Results will be filtered\n        to only those intersecting the bounding box.\n      intersects: A string or dictionary representing a GeoJSON geometry, or an object that implements a\n        __geo_interface__ property, as supported by several libraries including Shapely, ArcPy, PySAL, and geojson.\n        Results filtered to only those intersecting the geometry.\n      query: List or JSON of query parameters as per the STAC API query extension.\n      sortby: A single field or list of fields to sort the response by\n      max_retries:\n      delay:\n\n    Returns:\n    \"\"\"\n    if isinstance(collections, str):\n        collections = [collections]\n    if isinstance(sortby, dict):\n        sortby = [sortby]\n\n    intro_log = \"Initiating STAC API search\"\n    if query:\n        intro_log = f\"{intro_log} \\n\\tQuery : [{query}]\"\n    self.logger.info(intro_log)\n    items = []\n    for attempt in range(1, max_retries + 1):\n        try:\n            items = self._base_catalog_search(\n                date_range=date_range,\n                max_items=max_items,\n                limit=limit,\n                ids=ids,\n                collections=collections,\n                bbox=bbox,\n                intersects=intersects,\n                query=query,\n                sortby=sortby,\n            )\n        except APIError as e:  # pylint: disable=W0718\n            self.logger.error(f\"Attempt {attempt} failed: {e}\")\n            if attempt &lt; max_retries:\n                time.sleep(delay)\n            else:\n                raise e\n\n    if not items:\n        self.search_results = None\n\n    self.search_results = items\n    return items\n</code></pre>"},{"location":"reference/stac/#stac.StacSearch.search_for_date_ranges","title":"search_for_date_ranges","text":"<pre><code>search_for_date_ranges(\n    date_ranges: list[DateLike],\n    max_items: int | None = None,\n    limit: int | None = None,\n    collections: str | list | None = None,\n    bbox: BBoxLike | None = None,\n    intersects: IntersectsLike | None = None,\n    query: dict | None = None,\n    sortby: list | dict | None = None,\n    max_retries: int = 3,\n    delay=5,\n) -&gt; list\n</code></pre> <p>STAC API search that will use search query and parameters for each date range in given list of <code>date_ranges</code>.</p> <p>Date ranges can be generated with the help of the <code>geospatial_tools.utils.create_date_range_for_specific_period</code> function for more complex ranges.</p> <p>Parameter descriptions taken from pystac docs.</p> <p>Parameters:</p> Name Type Description Default <code>date_ranges</code> <code>list[DateLike]</code> <p>List containing datetime date ranges</p> required <code>max_items</code> <code>int | None</code> <p>The maximum number of items to return from the search, even if there are more matching results</p> <code>None</code> <code>limit</code> <code>int | None</code> <p>A recommendation to the service as to the number of items to return per page of results.</p> <code>None</code> <code>collections</code> <code>str | list | None</code> <p>List of one or more Collection IDs or pystac. Collection instances. Only Items in one of the provided Collections will be searched</p> <code>None</code> <code>bbox</code> <code>BBoxLike | None</code> <p>A list, tuple, or iterator representing a bounding box of 2D or 3D coordinates. Results will be filtered to only those intersecting the bounding box.</p> <code>None</code> <code>intersects</code> <code>IntersectsLike | None</code> <p>A string or dictionary representing a GeoJSON geometry, or an object that implements a geo_interface property, as supported by several libraries including Shapely, ArcPy, PySAL, and geojson. Results filtered to only those intersecting the geometry.</p> <code>None</code> <code>query</code> <code>dict | None</code> <p>List or JSON of query parameters as per the STAC API query extension.</p> <code>None</code> <code>sortby</code> <code>list | dict | None</code> <p>A single field or list of fields to sort the response by</p> <code>None</code> <code>max_retries</code> <code>int</code> <code>3</code> <code>delay</code> <code>5</code> <p>Returns:</p> Source code in <code>geospatial_tools/stac.py</code> <pre><code>def search_for_date_ranges(\n    self,\n    date_ranges: list[DateLike],\n    max_items: int | None = None,\n    limit: int | None = None,\n    collections: str | list | None = None,\n    bbox: geotools_types.BBoxLike | None = None,\n    intersects: geotools_types.IntersectsLike | None = None,\n    query: dict | None = None,\n    sortby: list | dict | None = None,\n    max_retries: int = 3,\n    delay=5,\n) -&gt; list:\n    \"\"\"\n    STAC API search that will use search query and parameters for each date range in given list of `date_ranges`.\n\n    Date ranges can be generated with the help of the `geospatial_tools.utils.create_date_range_for_specific_period`\n    function for more complex ranges.\n\n    Parameter descriptions taken from pystac docs.\n\n    Args:\n      date_ranges: List containing datetime date ranges\n      max_items: The maximum number of items to return from the search, even if there are more matching results\n      limit: A recommendation to the service as to the number of items to return per page of results.\n      collections: List of one or more Collection IDs or pystac. Collection instances. Only Items in one of the\n        provided Collections will be searched\n      bbox: A list, tuple, or iterator representing a bounding box of 2D or 3D coordinates. Results will be\n        filtered to only those intersecting the bounding box.\n      intersects: A string or dictionary representing a GeoJSON geometry, or an object that implements\n        a __geo_interface__ property, as supported by several libraries including Shapely, ArcPy, PySAL, and\n        geojson. Results filtered to only those intersecting the geometry.\n      query: List or JSON of query parameters as per the STAC API query extension.\n      sortby: A single field or list of fields to sort the response by\n      max_retries:\n      delay:\n\n    Returns:\n    \"\"\"\n    results = []\n    if isinstance(collections, str):\n        collections = [collections]\n    if isinstance(sortby, dict):\n        sortby = [sortby]\n\n    intro_log = f\"Running STAC API search for the following parameters: \\n\\tDate ranges : {date_ranges}\"\n    if query:\n        intro_log = f\"{intro_log} \\n\\tQuery : {query}\"\n    self.logger.info(intro_log)\n\n    for attempt in range(1, max_retries + 1):\n        try:\n            for date_range in date_ranges:\n                items = self._base_catalog_search(\n                    date_range=date_range,\n                    max_items=max_items,\n                    limit=limit,\n                    collections=collections,\n                    bbox=bbox,\n                    intersects=intersects,\n                    query=query,\n                    sortby=sortby,\n                )\n                results.extend(items)\n        except APIError as e:  # pylint: disable=W0718\n            self.logger.error(f\"Attempt {attempt} failed: {e}\")\n            if attempt &lt; max_retries:\n                time.sleep(delay)\n            else:\n                raise e\n\n    if not results:\n        self.logger.warning(f\"Search for date ranges [{date_ranges}] found no results!\")\n        self.search_results = None\n\n    self.search_results = results\n    return results\n</code></pre>"},{"location":"reference/stac/#stac.StacSearch.sort_results_by_cloud_coverage","title":"sort_results_by_cloud_coverage","text":"<pre><code>sort_results_by_cloud_coverage() -&gt; list | None\n</code></pre> <p>Sort results by cloud coverage.</p> Source code in <code>geospatial_tools/stac.py</code> <pre><code>def sort_results_by_cloud_coverage(self) -&gt; list | None:\n    \"\"\"Sort results by cloud coverage.\"\"\"\n    if self.search_results:\n        self.logger.debug(\"Sorting results by cloud cover (from least to most)\")\n        self.cloud_cover_sorted_results = sorted(\n            self.search_results, key=lambda item: item.properties.get(\"eo:cloud_cover\", float(\"inf\"))\n        )\n        return self.cloud_cover_sorted_results\n    self.logger.warning(\"No results found: please run a search before trying to sort results\")\n    return None\n</code></pre>"},{"location":"reference/stac/#stac.StacSearch.filter_no_data","title":"filter_no_data","text":"<pre><code>filter_no_data(property_name: str, max_no_data_value: int = 5) -&gt; list[Item] | None\n</code></pre> <p>Filter results and sorted results that are above a nodata value threshold.</p> <p>Parameters:</p> Name Type Description Default <code>property_name</code> <code>str</code> <p>str:</p> required <code>max_no_data_value</code> <code>int</code> <p>int:  (Default value = 5)</p> <code>5</code> <p>Returns:</p> Source code in <code>geospatial_tools/stac.py</code> <pre><code>def filter_no_data(self, property_name: str, max_no_data_value: int = 5) -&gt; list[pystac.Item] | None:\n    \"\"\"\n    Filter results and sorted results that are above a nodata value threshold.\n\n    Args:\n      property_name: str:\n      max_no_data_value: int:  (Default value = 5)\n\n    Returns:\n    \"\"\"\n    sorted_results = self.cloud_cover_sorted_results\n    if not sorted_results:\n        sorted_results = self.sort_results_by_cloud_coverage()\n    if not sorted_results:\n        return None\n\n    filtered_results = []\n    for item in sorted_results:\n        if item.properties[property_name] &lt; max_no_data_value:\n            filtered_results.append(item)\n    self.filtered_results = filtered_results\n\n    return filtered_results\n</code></pre>"},{"location":"reference/stac/#stac.StacSearch.download_search_results","title":"download_search_results","text":"<pre><code>download_search_results(bands: list, base_directory: str | Path) -&gt; list[Asset]\n</code></pre> <p>Parameters:</p> Name Type Description Default <code>bands</code> <code>list</code> <p>List of bands to download from asset</p> required <code>base_directory</code> <code>str | Path</code> <p>Base directory where assets will be downloaded</p> required <p>Returns:</p> Source code in <code>geospatial_tools/stac.py</code> <pre><code>def download_search_results(self, bands: list, base_directory: str | Path) -&gt; list[Asset]:\n    \"\"\"\n\n    Args:\n      bands: List of bands to download from asset\n      base_directory: Base directory where assets will be downloaded\n\n    Returns:\n\n\n    \"\"\"\n    downloaded_search_results = self._download_results(\n        results=self.search_results, bands=bands, base_directory=base_directory\n    )\n    self.downloaded_search_assets = downloaded_search_results\n    return downloaded_search_results\n</code></pre>"},{"location":"reference/stac/#stac.StacSearch.download_sorted_by_cloud_cover_search_results","title":"download_sorted_by_cloud_cover_search_results","text":"<pre><code>download_sorted_by_cloud_cover_search_results(\n    bands: list, base_directory: str | Path, first_x_num_of_items: int | None = None\n) -&gt; list[Asset]\n</code></pre> <p>Parameters:</p> Name Type Description Default <code>bands</code> <code>list</code> <p>List of bands to download from asset</p> required <code>base_directory</code> <code>str | Path</code> <p>Base directory where assets will be downloaded</p> required <code>first_x_num_of_items</code> <code>int | None</code> <p>Number of items to download from the results</p> <code>None</code> <p>Returns:</p> Source code in <code>geospatial_tools/stac.py</code> <pre><code>def download_sorted_by_cloud_cover_search_results(\n    self, bands: list, base_directory: str | Path, first_x_num_of_items: int | None = None\n) -&gt; list[Asset]:\n    \"\"\"\n\n    Args:\n      bands: List of bands to download from asset\n      base_directory: Base directory where assets will be downloaded\n      first_x_num_of_items: Number of items to download from the results\n\n    Returns:\n\n\n    \"\"\"\n    results = self._generate_best_results()\n    if not results:\n        return []\n    if first_x_num_of_items:\n        results = results[:first_x_num_of_items]\n    downloaded_search_results = self._download_results(results=results, bands=bands, base_directory=base_directory)\n    self.downloaded_cloud_cover_sorted_assets = downloaded_search_results\n    return downloaded_search_results\n</code></pre>"},{"location":"reference/stac/#stac.StacSearch.download_best_cloud_cover_result","title":"download_best_cloud_cover_result","text":"<pre><code>download_best_cloud_cover_result(\n    bands: list, base_directory: str | Path\n) -&gt; Asset | None\n</code></pre> <p>Parameters:</p> Name Type Description Default <code>bands</code> <code>list</code> <p>List of bands to download from asset</p> required <code>base_directory</code> <code>str | Path</code> <p>Base directory where assets will be downloaded</p> required <p>Returns:</p> Source code in <code>geospatial_tools/stac.py</code> <pre><code>def download_best_cloud_cover_result(self, bands: list, base_directory: str | Path) -&gt; Asset | None:\n    \"\"\"\n\n    Args:\n      bands: List of bands to download from asset\n      base_directory: Base directory where assets will be downloaded\n\n    Returns:\n\n\n    \"\"\"\n    results = self._generate_best_results()\n    best_result = results[0]\n    best_result = [best_result]\n\n    if self.downloaded_cloud_cover_sorted_assets:\n        self.logger.info(f\"Asset [{best_result[0].id}] is already downloaded\")\n        self.downloaded_best_sorted_asset = self.downloaded_cloud_cover_sorted_assets[0]\n        return self.downloaded_cloud_cover_sorted_assets[0]\n\n    downloaded_search_results = self._download_results(\n        results=best_result, bands=bands, base_directory=base_directory\n    )\n    self.downloaded_best_sorted_asset = downloaded_search_results[0]\n    return downloaded_search_results[0]\n</code></pre>"},{"location":"reference/stac/#stac.create_planetary_computer_catalog","title":"create_planetary_computer_catalog","text":"<pre><code>create_planetary_computer_catalog(\n    max_retries: int = 3, delay=5, logger=LOGGER\n) -&gt; Client | None\n</code></pre> <p>Creates a Planetary Computer Catalog Client.</p> <p>Parameters:</p> Name Type Description Default <code>max_retries</code> <code>int</code> <p>(Default value = 3)</p> <code>3</code> <code>delay</code> <p>(Default value = 5)</p> <code>5</code> <code>logger</code> <p>(Default value = LOGGER)</p> <code>LOGGER</code> <p>Returns:</p> Source code in <code>geospatial_tools/stac.py</code> <pre><code>def create_planetary_computer_catalog(max_retries: int = 3, delay=5, logger=LOGGER) -&gt; pystac_client.Client | None:\n    \"\"\"\n    Creates a Planetary Computer Catalog Client.\n\n    Args:\n      max_retries:  (Default value = 3)\n      delay:  (Default value = 5)\n      logger:  (Default value = LOGGER)\n\n    Returns:\n    \"\"\"\n    for attempt in range(1, max_retries + 1):\n        try:\n            client = pystac_client.Client.open(PLANETARY_COMPUTER_API, modifier=sign_inplace)\n            logger.debug(\"Successfully connected to the API.\")\n            return client\n        except Exception as e:  # pylint: disable=W0718\n            logger.error(f\"Attempt {attempt} failed: {e}\")\n            if attempt &lt; max_retries:\n                time.sleep(delay)\n            else:\n                logger.error(e)\n                raise e\n    return None\n</code></pre>"},{"location":"reference/stac/#stac.catalog_generator","title":"catalog_generator","text":"<pre><code>catalog_generator(catalog_name, logger=LOGGER) -&gt; Client | None\n</code></pre> <p>Parameters:</p> Name Type Description Default <code>catalog_name</code> required <code>logger</code> <code>LOGGER</code> <p>Returns:</p> Type Description <code>Client | None</code> <p>STAC Client</p> Source code in <code>geospatial_tools/stac.py</code> <pre><code>def catalog_generator(catalog_name, logger=LOGGER) -&gt; pystac_client.Client | None:\n    \"\"\"\n\n    Args:\n      catalog_name:\n      logger:\n\n    Returns:\n        STAC Client\n    \"\"\"\n    catalog_dict = {PLANETARY_COMPUTER: create_planetary_computer_catalog}\n    if catalog_name not in catalog_dict:\n        logger.error(f\"Unsupported catalog name: {catalog_name}\")\n        return None\n\n    catalog = catalog_dict[catalog_name]()\n\n    return catalog\n</code></pre>"},{"location":"reference/stac/#stac.list_available_catalogs","title":"list_available_catalogs","text":"<pre><code>list_available_catalogs(logger: Logger = LOGGER) -&gt; frozenset[str]\n</code></pre> <p>Parameters:</p> Name Type Description Default <code>logger</code> <code>Logger</code> <code>LOGGER</code> <p>Returns:</p> Source code in <code>geospatial_tools/stac.py</code> <pre><code>def list_available_catalogs(logger: logging.Logger = LOGGER) -&gt; frozenset[str]:\n    \"\"\"\n\n    Args:\n      logger:\n\n    Returns:\n\n\n    \"\"\"\n    logger.info(\"Available catalogs\")\n    return CATALOG_NAME_LIST\n</code></pre>"},{"location":"reference/utils/","title":"utils","text":"<p>This module contains general utility functions.</p>"},{"location":"reference/utils/#utils.create_logger","title":"create_logger","text":"<pre><code>create_logger(logger_name: str) -&gt; Logger\n</code></pre> <p>Creates a logger object using input name parameter that outputs to stdout.</p> <p>Parameters:</p> Name Type Description Default <code>logger_name</code> <code>str</code> <p>Name of logger</p> required <p>Returns:</p> Source code in <code>geospatial_tools/utils.py</code> <pre><code>def create_logger(logger_name: str) -&gt; logging.Logger:\n    \"\"\"\n    Creates a logger object using input name parameter that outputs to stdout.\n\n    Args:\n      logger_name: Name of logger\n\n    Returns:\n    \"\"\"\n    logging_level = logging.INFO\n    app_config_path = CONFIGS / \"geospatial_tools_ini.yaml\"\n    if app_config_path.exists():\n        with app_config_path.open(\"r\", encoding=\"UTF-8\") as config_file:\n            application_params = yaml.safe_load(config_file)\n            logger_params = application_params[\"logging\"]\n            logging_level = logger_params[\"logging_level\"].upper()\n    if os.getenv(\"GEO_LOG_LEVEL\"):\n        logging_level = os.getenv(\"GEO_LOG_LEVEL\").upper()\n\n    logger = logging.getLogger(logger_name)\n    logger.setLevel(logging_level)\n    handler = logging.StreamHandler(sys.stdout)\n    formatter = logging.Formatter(\n        fmt=\"[%(asctime)s] %(levelname)-10.10s [%(threadName)s][%(name)s] %(message)s\",\n        datefmt=\"%Y-%m-%d %H:%M:%S\",\n    )\n    handler.setFormatter(formatter)\n    logger.addHandler(handler)\n    return logger\n</code></pre>"},{"location":"reference/utils/#utils.get_yaml_config","title":"get_yaml_config","text":"<pre><code>get_yaml_config(yaml_config_file: str, logger: Logger = LOGGER) -&gt; dict\n</code></pre> <p>This function takes in the path, or name of the file if it can be found in the config/ folder, with of without the extension, and returns the values of the file in a dictionary format.</p> <p>Ex. For a file named app_config.yml (or app_config.yaml), directly in the config/ folder,     the function could be called like so : <code>params = get_yaml_config('app_config')</code></p> <p>Parameters:</p> Name Type Description Default <code>yaml_config_file</code> <code>str</code> <p>Path to yaml config file. If config file is in the config folder, you can use the file's name without the extension.</p> required <code>logger</code> <code>Logger</code> <p>Logger to handle messaging, by default LOGGER</p> <code>LOGGER</code> <p>Returns:</p> Source code in <code>geospatial_tools/utils.py</code> <pre><code>def get_yaml_config(yaml_config_file: str, logger: logging.Logger = LOGGER) -&gt; dict:\n    \"\"\"\n    This function takes in the path, or name of the file if it can be found in the config/ folder, with of without the\n    extension, and returns the values of the file in a dictionary format.\n\n    Ex. For a file named app_config.yml (or app_config.yaml), directly in the config/ folder,\n        the function could be called like so : `params = get_yaml_config('app_config')`\n\n    Args:\n      yaml_config_file: Path to yaml config file. If config file is in the config folder,\n        you can use the file's name without the extension.\n      logger: Logger to handle messaging, by default LOGGER\n\n    Returns:\n    \"\"\"\n\n    potential_paths = [\n        Path(yaml_config_file),\n        CONFIGS / yaml_config_file,\n        CONFIGS / f\"{yaml_config_file}.yaml\",\n        CONFIGS / f\"{yaml_config_file}.yml\",\n    ]\n\n    config_filepath = None\n    for path in potential_paths:\n        if path.exists():\n            config_filepath = path\n            logger.info(f\"Yaml config file [{path!s}] found.\")\n            break\n\n    params = {}\n    if not config_filepath:\n        logger.error(f\"Yaml config file [{yaml_config_file}] was not found.\")\n        return params\n\n    try:\n        with config_filepath.open(\"r\", encoding=\"UTF-8\") as file:\n            logger.info(f\"Loading YAML config file [{config_filepath}].\")\n            return yaml.safe_load(file)\n    except yaml.YAMLError as e:\n        logger.warning(f\"Error loading YAML file [{config_filepath}]: {e}\")\n        return {}\n</code></pre>"},{"location":"reference/utils/#utils.get_json_config","title":"get_json_config","text":"<pre><code>get_json_config(json_config_file: str, logger=LOGGER) -&gt; dict\n</code></pre> <p>This function takes in the path, or name of the file if it can be found in the config/ folder, with of without the extension, and returns the values of the file in a dictionary format.</p> <p>Ex. For a file named app_config.json, directly in the config/ folder,     the function could be called like so : <code>params = get_json_config('app_config')</code></p> <p>Parameters:</p> Name Type Description Default <code>json_config_file</code> <code>str</code> <p>Path to JSON config file. If config file is in the config folder,</p> required <code>logger</code> <p>Logger to handle messaging</p> <code>LOGGER</code> <p>Returns:</p> Source code in <code>geospatial_tools/utils.py</code> <pre><code>def get_json_config(json_config_file: str, logger=LOGGER) -&gt; dict:\n    \"\"\"\n    This function takes in the path, or name of the file if it can be found in the config/ folder, with of without the\n    extension, and returns the values of the file in a dictionary format.\n\n    Ex. For a file named app_config.json, directly in the config/ folder,\n        the function could be called like so : `params = get_json_config('app_config')`\n\n    Args:\n      json_config_file: Path to JSON config file. If config file is in the config folder,\n      logger: Logger to handle messaging\n\n    Returns:\n    \"\"\"\n\n    potential_paths = [\n        Path(json_config_file),\n        CONFIGS / json_config_file,\n        CONFIGS / f\"{json_config_file}.json\",\n    ]\n\n    config_filepath = None\n    for path in potential_paths:\n        if path.exists():\n            config_filepath = path\n            logger.info(f\"JSON config file [{path!s}] found.\")\n            break\n\n    if not config_filepath:\n        logger.error(f\"JSON config file [{json_config_file}] not found.\")\n        return {}\n\n    try:\n        with config_filepath.open(\"r\", encoding=\"UTF-8\") as file:\n            logger.info(f\"Loading JSON config file [{config_filepath}].\")\n            return json.load(file)\n    except json.JSONDecodeError as e:\n        logger.warning(f\"Error loading JSON file [{config_filepath}]: {e}\")\n        return {}\n</code></pre>"},{"location":"reference/utils/#utils.create_crs","title":"create_crs","text":"<pre><code>create_crs(dataset_crs: str | int, logger=LOGGER)\n</code></pre> <p>Parameters:</p> Name Type Description Default <code>dataset_crs</code> <code>str | int</code> <p>EPSG code in string or int format. Can be given in the following ways: 5070 | \"5070\" | \"EPSG:5070\"</p> required <code>logger</code> <p>Logger instance (Default value = LOGGER)</p> <code>LOGGER</code> <p>Returns:</p> Source code in <code>geospatial_tools/utils.py</code> <pre><code>def create_crs(dataset_crs: str | int, logger=LOGGER):\n    \"\"\"\n\n    Args:\n      dataset_crs: EPSG code in string or int format. Can be given in the following ways: 5070 | \"5070\" | \"EPSG:5070\"\n      logger: Logger instance (Default value = LOGGER)\n\n    Returns:\n\n\n    \"\"\"\n    logger.info(f\"Creating EPSG code from following input : [{dataset_crs}]\")\n    is_int = isinstance(dataset_crs, int) or dataset_crs.isnumeric()\n    is_str = isinstance(dataset_crs, str)\n    contains_epsg = is_str and \"EPSG:\" in dataset_crs\n    if is_int:\n        return CRS.from_epsg(dataset_crs)\n    if contains_epsg:\n        return CRS.from_string(dataset_crs.upper())\n    if \":\" in dataset_crs:\n        logger.warning(\"Input is not conform to standards. Attempting to extract code from the provided input.\")\n        recovered_code = dataset_crs.split(\":\")[-1]\n        if recovered_code.isnumeric():\n            return CRS.from_epsg(recovered_code)\n\n    logger.error(f\"Encountered problem while trying to format EPSG code from input : [{dataset_crs}]\")\n    return None\n</code></pre>"},{"location":"reference/utils/#utils.download_url","title":"download_url","text":"<pre><code>download_url(\n    url: str, filename: str | Path, overwrite: bool = False, logger=LOGGER\n) -&gt; Path | None\n</code></pre> <p>This function downloads a file from a given URL.</p> <p>Parameters:</p> Name Type Description Default <code>url</code> <code>str</code> <p>Url to download</p> required <code>filename</code> <code>str | Path</code> <p>Filename (or full path) to save the downloaded file</p> required <code>overwrite</code> <code>bool</code> <p>If True, overwrite existing file</p> <code>False</code> <code>logger</code> <p>Logger instance</p> <code>LOGGER</code> <p>Returns:</p> Source code in <code>geospatial_tools/utils.py</code> <pre><code>def download_url(url: str, filename: str | Path, overwrite: bool = False, logger=LOGGER) -&gt; Path | None:\n    \"\"\"\n    This function downloads a file from a given URL.\n\n    Args:\n      url: Url to download\n      filename: Filename (or full path) to save the downloaded file\n      overwrite: If True, overwrite existing file\n      logger: Logger instance\n\n    Returns:\n    \"\"\"\n    if isinstance(filename, str):\n        filename = Path(filename)\n\n    if filename.exists() and not overwrite:\n        logger.info(f\"File [{filename}] already exists. Skipping download.\")\n        return filename\n\n    response = requests.get(url, timeout=None)\n    if response.status_code == 200:\n        with open(filename, \"wb\") as f:\n            f.write(response.content)\n        logger.info(f\"Downloaded {filename} successfully.\")\n        return filename\n\n    logger.error(f\"Failed to download the asset. Status code: {response.status_code}\")\n    return None\n</code></pre>"},{"location":"reference/utils/#utils.unzip_file","title":"unzip_file","text":"<pre><code>unzip_file(\n    zip_path: str | Path, extract_to: str | Path, logger: Logger = LOGGER\n) -&gt; list[str | Path]\n</code></pre> <p>This function unzips an archive to a specific directory.</p> <p>Parameters:</p> Name Type Description Default <code>zip_path</code> <code>str | Path</code> <p>Path to zip file</p> required <code>extract_to</code> <code>str | Path</code> <p>Path of directory to extract the zip file</p> required <code>logger</code> <code>Logger</code> <p>Logger instance</p> <code>LOGGER</code> <p>Returns:</p> Source code in <code>geospatial_tools/utils.py</code> <pre><code>def unzip_file(zip_path: str | Path, extract_to: str | Path, logger: logging.Logger = LOGGER) -&gt; list[str | Path]:\n    \"\"\"\n    This function unzips an archive to a specific directory.\n\n    Args:\n      zip_path: Path to zip file\n      extract_to: Path of directory to extract the zip file\n      logger: Logger instance\n\n    Returns:\n    \"\"\"\n    if isinstance(zip_path, str):\n        zip_path = Path(zip_path)\n    if isinstance(extract_to, str):\n        extract_to = Path(extract_to)\n    extract_to.mkdir(parents=True, exist_ok=True)\n    extracted_files = []\n    with zipfile.ZipFile(zip_path, \"r\") as zip_ref:\n        for member in zip_ref.infolist():\n            zip_ref.extract(member, extract_to)\n            logger.info(f\"Extracted: [{member.filename}]\")\n            extracted_files.append(f\"{extract_to}/{member.filename}\")\n    return extracted_files\n</code></pre>"},{"location":"reference/utils/#utils.create_date_range_for_specific_period","title":"create_date_range_for_specific_period","text":"<pre><code>create_date_range_for_specific_period(\n    start_year: int, end_year: int, start_month_range: int, end_month_range: int\n) -&gt; list[str]\n</code></pre> <p>This function create a list of date ranges.</p> <p>For example, I want to create date ranges for 2020 and 2021, but only for the months from March to May. I therefore expect to have 2 ranges: [2020-03-01 to 2020-05-30, 2021-03-01 to 2021-05-30].</p> <p>Handles the automatic definition of the last day for the end month, as well as periods that cross over years</p> <p>For example, I want to create date ranges for 2020 and 2022, but only for the months from November to January. I therefore expect to have 2 ranges: [2020-11-01 to 2021-01-31, 2021-11-01 to 2022-01-31].</p> <p>Parameters:</p> Name Type Description Default <code>start_year</code> <code>int</code> <p>Start year for ranges</p> required <code>end_year</code> <code>int</code> <p>End year for ranges</p> required <code>start_month_range</code> <code>int</code> <p>Starting month for each period</p> required <code>end_month_range</code> <code>int</code> <p>End month for each period (inclusively)</p> required <p>Returns:</p> Source code in <code>geospatial_tools/utils.py</code> <pre><code>def create_date_range_for_specific_period(\n    start_year: int, end_year: int, start_month_range: int, end_month_range: int\n) -&gt; list[str]:\n    \"\"\"\n    This function create a list of date ranges.\n\n    For example, I want to create date ranges for 2020 and 2021, but only for the months from March to May.\n    I therefore expect to have 2 ranges: [2020-03-01 to 2020-05-30, 2021-03-01 to 2021-05-30].\n\n    Handles the automatic definition of the last day for the end month, as well as periods that cross over years\n\n    For example, I want to create date ranges for 2020 and 2022, but only for the months from November to January.\n    I therefore expect to have 2 ranges: [2020-11-01 to 2021-01-31, 2021-11-01 to 2022-01-31].\n\n    Args:\n      start_year: Start year for ranges\n      end_year: End year for ranges\n      start_month_range: Starting month for each period\n      end_month_range: End month for each period (inclusively)\n\n    Returns:\n    \"\"\"\n    date_ranges = []\n    year_bump = 0\n    if start_month_range &gt; end_month_range:\n        year_bump = 1\n    range_end_year = end_year + 1 - year_bump\n    for year in range(start_year, range_end_year):\n        start_date = datetime.datetime(year, start_month_range, 1)\n        last_day = calendar.monthrange(year + year_bump, end_month_range)[1]\n        end_date = datetime.datetime(year + year_bump, end_month_range, last_day, 23, 59, 59)\n        date_ranges.append(f\"{start_date.isoformat()}Z/{end_date.isoformat()}Z\")\n    return date_ranges\n</code></pre>"},{"location":"reference/utils/#utils.parse_gzip_header","title":"parse_gzip_header","text":"<pre><code>parse_gzip_header(path: str | Path) -&gt; dict[str, Any]\n</code></pre> <p>Parse the gzip header at the beginning of <code>path</code> (first member only).</p> <p>Raises ValueError if file doesn't look like gzip.</p> <p>Parameters:</p> Name Type Description Default <code>path</code> <code>str | Path</code> <p>Path to gzip file</p> required <p>Returns:</p> Name Type Description <code>dict</code> <code>dict[str, Any]</code> <p>Returns a dict with keys       - compression_method (int)       - flags (int)       - mtime (int, Unix epoch or 0)       - xflags (int)       - os (int)       - original_name (Optional[str])   # FNAME       - comment      (Optional[str])    # FCOMMENT       - header_end_offset (int)         # file offset where compressed data starts</p> Source code in <code>geospatial_tools/utils.py</code> <pre><code>def parse_gzip_header(path: str | Path) -&gt; dict[str, Any]:\n    \"\"\"\n    Parse the gzip header at the beginning of `path` (first member only).\n\n    Raises ValueError if file doesn't look like gzip.\n\n    Args:\n        path: Path to gzip file\n\n    Returns:\n        dict: Returns a dict with keys\n                  - compression_method (int)\n                  - flags (int)\n                  - mtime (int, Unix epoch or 0)\n                  - xflags (int)\n                  - os (int)\n                  - original_name (Optional[str])   # FNAME\n                  - comment      (Optional[str])    # FCOMMENT\n                  - header_end_offset (int)         # file offset where compressed data starts\n    \"\"\"\n    p = Path(path)\n    with p.open(\"rb\") as f:\n        # Magic\n        if f.read(2) != b\"\\x1f\\x8b\":\n            raise ValueError(f\"{p} is not a gzip file (bad magic)\")\n\n        method_b = f.read(1)\n        flags_b = f.read(1)\n        if not method_b or not flags_b:\n            raise ValueError(\"Truncated header\")\n\n        compression_method = method_b[0]\n        flags = flags_b[0]\n\n        # MTIME(4), XFL(1), OS(1)\n        mtime_bytes = f.read(4)\n        if len(mtime_bytes) != 4:\n            raise ValueError(\"Truncated header (mtime)\")\n        mtime = struct.unpack(\"&lt;I\", mtime_bytes)[0]\n        xflags = f.read(1)[0]\n        os_code = f.read(1)[0]\n\n        # Optional fields in order\n        if flags &amp; FEXTRA:\n            xlen_bytes = f.read(2)\n            if len(xlen_bytes) != 2:\n                raise ValueError(\"Truncated FEXTRA length\")\n            xlen = struct.unpack(\"&lt;H\", xlen_bytes)[0]\n            _ = f.read(xlen)  # skip payload\n\n        original_name: Optional[str] = None\n        if flags &amp; FNAME:\n            # Historically ISO-8859-1; utf-8 with replace is pragmatic\n            original_name = _read_cstring(f).decode(\"utf-8\", errors=\"replace\")\n\n        comment: Optional[str] = None\n        if flags &amp; FCOMMENT:\n            comment = _read_cstring(f).decode(\"utf-8\", errors=\"replace\")\n\n        if flags &amp; FHCRC:\n            _ = f.read(2)  # skip header CRC16\n\n        return {\n            \"compression_method\": compression_method,\n            \"flags\": flags,\n            \"mtime\": mtime,\n            \"xflags\": xflags,\n            \"os\": os_code,\n            \"original_name\": original_name,\n            \"comment\": comment,\n            \"header_end_offset\": f.tell(),\n        }\n</code></pre>"},{"location":"reference/vector/","title":"vector","text":"<p>This module contains functions that process or create vector data.</p>"},{"location":"reference/vector/#vector.create_grid_coordinates","title":"create_grid_coordinates","text":"<pre><code>create_grid_coordinates(\n    bounding_box: list | tuple, grid_size: float, logger: Logger = LOGGER\n) -&gt; tuple[ndarray, ndarray]\n</code></pre> <p>Create grid coordinates based on input bounding box and grid size.</p> <p>Parameters:</p> Name Type Description Default <code>bounding_box</code> <code>list | tuple</code> <p>The bounding box of the grid as (min_lon, min_lat, max_lon, max_lat). Unit needs to be based on projection used (meters, degrees, etc.).</p> required <code>grid_size</code> <code>float</code> <p>Cell size for grid. Unit needs to be based on projection used (meters, degrees, etc.).</p> required <code>logger</code> <code>Logger</code> <p>Logger instance.</p> <code>LOGGER</code> <p>Returns:</p> Source code in <code>geospatial_tools/vector.py</code> <pre><code>def create_grid_coordinates(\n    bounding_box: list | tuple, grid_size: float, logger: logging.Logger = LOGGER\n) -&gt; tuple[ndarray, ndarray]:\n    \"\"\"\n    Create grid coordinates based on input bounding box and grid size.\n\n    Args:\n      bounding_box: The bounding box of the grid as (min_lon, min_lat, max_lon, max_lat).\n        Unit needs to be based on projection used (meters, degrees, etc.).\n      grid_size: Cell size for grid. Unit needs to be based on projection used (meters, degrees, etc.).\n      logger: Logger instance.\n\n    Returns:\n    \"\"\"\n    logger.info(f\"Creating grid coordinates for bounding box [{bounding_box}]\")\n    min_lon, min_lat, max_lon, max_lat = bounding_box\n    lon_coords = np.arange(start=min_lon, stop=max_lon, step=grid_size)\n    lat_coords = np.arange(start=min_lat, stop=max_lat, step=grid_size)\n    return lon_coords, lat_coords\n</code></pre>"},{"location":"reference/vector/#vector.generate_flattened_grid_coords","title":"generate_flattened_grid_coords","text":"<pre><code>generate_flattened_grid_coords(\n    lon_coords: ndarray, lat_coords: ndarray, logger: Logger = LOGGER\n) -&gt; tuple[ndarray, ndarray]\n</code></pre> <p>Takes in previously created grid coordinates and flattens them.</p> <p>Parameters:</p> Name Type Description Default <code>lon_coords</code> <code>ndarray</code> <p>Longitude grid coordinates</p> required <code>lat_coords</code> <code>ndarray</code> <p>Latitude grid coordinates</p> required <code>logger</code> <code>Logger</code> <p>Logger instance.</p> <code>LOGGER</code> <p>Returns:</p> Source code in <code>geospatial_tools/vector.py</code> <pre><code>def generate_flattened_grid_coords(\n    lon_coords: ndarray, lat_coords: ndarray, logger: logging.Logger = LOGGER\n) -&gt; tuple[ndarray, ndarray]:\n    \"\"\"\n    Takes in previously created grid coordinates and flattens them.\n\n    Args:\n      lon_coords: Longitude grid coordinates\n      lat_coords: Latitude grid coordinates\n      logger: Logger instance.\n\n    Returns:\n    \"\"\"\n\n    logger.info(\"Creating flattened grid coordinates\")\n    lon_grid, lat_grid = np.meshgrid(lon_coords, lat_coords)\n    lon_grid = lon_grid.flatten()\n    lat_grid = lat_grid.flatten()\n    return lon_grid, lat_grid\n</code></pre>"},{"location":"reference/vector/#vector.create_vector_grid","title":"create_vector_grid","text":"<pre><code>create_vector_grid(\n    bounding_box: list | tuple,\n    grid_size: float,\n    crs: str = None,\n    logger: Logger = LOGGER,\n) -&gt; GeoDataFrame\n</code></pre> <p>Create a grid of polygons within the specified bounds and cell size. This function uses NumPy vectorized arrays for optimized performance.</p> <p>Parameters:</p> Name Type Description Default <code>bounding_box</code> <code>list | tuple</code> <p>The bounding box of the grid as (min_lon, min_lat, max_lon, max_lat).</p> required <code>grid_size</code> <code>float</code> <p>The size of each grid cell in degrees.</p> required <code>crs</code> <code>str</code> <p>CRS code for projection. ex. 'EPSG:4326'</p> <code>None</code> <code>logger</code> <code>Logger</code> <p>Logger instance.</p> <code>LOGGER</code> <p>Returns:</p> Source code in <code>geospatial_tools/vector.py</code> <pre><code>def create_vector_grid(\n    bounding_box: list | tuple, grid_size: float, crs: str = None, logger: logging.Logger = LOGGER\n) -&gt; GeoDataFrame:\n    \"\"\"\n    Create a grid of polygons within the specified bounds and cell size. This function uses NumPy vectorized arrays for\n    optimized performance.\n\n    Args:\n      bounding_box: The bounding box of the grid as (min_lon, min_lat, max_lon, max_lat).\n      grid_size: The size of each grid cell in degrees.\n      crs: CRS code for projection. ex. 'EPSG:4326'\n      logger: Logger instance.\n\n    Returns:\n    \"\"\"\n    lon_coords, lat_coords = create_grid_coordinates(bounding_box=bounding_box, grid_size=grid_size, logger=logger)\n    lon_flat_grid, lat_flat_grid = generate_flattened_grid_coords(\n        lat_coords=lat_coords, lon_coords=lon_coords, logger=logger\n    )\n\n    num_cells = len(lon_flat_grid)\n    logger.info(f\"Allocating polygon array for [{num_cells}] polygons\")\n    polygons = np.empty(num_cells, dtype=object)\n\n    for i in range(num_cells):\n        x, y = lon_flat_grid[i], lat_flat_grid[i]\n        polygons[i] = Polygon([(x, y), (x + grid_size, y), (x + grid_size, y + grid_size), (x, y + grid_size)])\n\n    properties = {\"data\": {\"geometry\": polygons}}\n    if crs:\n        properties[\"crs\"] = crs\n    grid = GeoDataFrame(**properties)\n    grid.sindex  # pylint: disable=W0104\n    _generate_uuid_column(grid)\n    return grid\n</code></pre>"},{"location":"reference/vector/#vector.create_vector_grid_parallel","title":"create_vector_grid_parallel","text":"<pre><code>create_vector_grid_parallel(\n    bounding_box: list | tuple,\n    grid_size: float,\n    crs: str | int = None,\n    num_of_workers: int = None,\n    logger: Logger = LOGGER,\n) -&gt; GeoDataFrame\n</code></pre> <p>Create a grid of polygons within the specified bounds and cell size. This function uses NumPy for optimized performance and ProcessPoolExecutor for parallel execution.</p> <p>Parameters:</p> Name Type Description Default <code>bounding_box</code> <code>list | tuple</code> <p>The bounding box of the grid as (min_lon, min_lat, max_lon, max_lat).</p> required <code>grid_size</code> <code>float</code> <p>The size of each grid cell in degrees.</p> required <code>crs</code> <code>str | int</code> <p>Coordinate reference system for the resulting GeoDataFrame.</p> <code>None</code> <code>num_of_workers</code> <code>int</code> <p>The number of processes to use for parallel execution. Defaults to the min of number of CPU cores or number of cells in the grid</p> <code>None</code> <code>logger</code> <code>Logger</code> <p>Logger instance.</p> <code>LOGGER</code> <p>Returns:</p> Source code in <code>geospatial_tools/vector.py</code> <pre><code>def create_vector_grid_parallel(\n    bounding_box: list | tuple,\n    grid_size: float,\n    crs: str | int = None,\n    num_of_workers: int = None,\n    logger: logging.Logger = LOGGER,\n) -&gt; GeoDataFrame:\n    \"\"\"\n    Create a grid of polygons within the specified bounds and cell size. This function uses NumPy for optimized\n    performance and ProcessPoolExecutor for parallel execution.\n\n    Args:\n      bounding_box: The bounding box of the grid as (min_lon, min_lat, max_lon, max_lat).\n      grid_size: The size of each grid cell in degrees.\n      crs: Coordinate reference system for the resulting GeoDataFrame.\n      num_of_workers: The number of processes to use for parallel execution. Defaults to the min of number of CPU cores\n        or number of cells in the grid\n      logger: Logger instance.\n\n    Returns:\n    \"\"\"\n    lon_coords, lat_coords = create_grid_coordinates(bounding_box=bounding_box, grid_size=grid_size, logger=logger)\n    lon_flat_grid, lat_flat_grid = generate_flattened_grid_coords(\n        lat_coords=lat_coords, lon_coords=lon_coords, logger=logger\n    )\n\n    num_cells = len(lon_flat_grid)\n    workers = min(cpu_count(), num_cells)\n    if num_of_workers:\n        workers = num_of_workers\n\n    logger.info(f\"Number of workers used: {workers}\")\n    logger.info(f\"Allocating polygon array for [{num_cells}] polygons\")\n\n    chunk_size = (num_cells + workers - 1) // workers\n    chunks = [\n        (lon_flat_grid[i : i + chunk_size], lat_flat_grid[i : i + chunk_size], grid_size)\n        for i in range(0, num_cells, chunk_size)\n    ]\n\n    polygons = []\n    logger.info(\"Creating polygons from chunks\")\n    with ProcessPoolExecutor(max_workers=workers) as executor:\n        results = executor.map(_create_polygons_from_coords_chunk, chunks)\n        for result in results:\n            polygons.extend(result)\n\n    logger.info(\"Managing properties\")\n    properties = {\"data\": {\"geometry\": polygons}}\n    if crs:\n        projection = create_crs(crs)\n        properties[\"crs\"] = projection\n    grid: GeoDataFrame = GeoDataFrame(**properties)\n    logger.info(\"Creating spatial index\")\n    grid.sindex  # pylint: disable=W0104\n    logger.info(\"Generating polygon UUIDs\")\n    _generate_uuid_column(grid)\n    return grid\n</code></pre>"},{"location":"reference/vector/#vector.dask_spatial_join","title":"dask_spatial_join","text":"<pre><code>dask_spatial_join(\n    select_features_from: GeoDataFrame,\n    intersected_with: GeoDataFrame,\n    join_type: str = \"inner\",\n    predicate: str = \"intersects\",\n    num_of_workers=4,\n) -&gt; GeoDataFrame\n</code></pre> <p>Parameters:</p> Name Type Description Default <code>select_features_from</code> <code>GeoDataFrame</code> required <code>intersected_with</code> <code>GeoDataFrame</code> required <code>join_type</code> <code>str</code> <p>str:</p> <code>'inner'</code> <code>predicate</code> <code>str</code> <p>str:</p> <code>'intersects'</code> <code>num_of_workers</code> <code>4</code> <p>Returns:</p> Source code in <code>geospatial_tools/vector.py</code> <pre><code>def dask_spatial_join(\n    select_features_from: GeoDataFrame,\n    intersected_with: GeoDataFrame,\n    join_type: str = \"inner\",\n    predicate: str = \"intersects\",\n    num_of_workers=4,\n) -&gt; GeoDataFrame:\n    \"\"\"\n\n    Args:\n      select_features_from:\n      intersected_with:\n      join_type: str:\n      predicate: str:\n      num_of_workers:\n\n    Returns:\n\n\n    \"\"\"\n    dask_select_gdf = dgpd.from_geopandas(select_features_from, npartitions=num_of_workers)\n    dask_intersected_gdf = dgpd.from_geopandas(intersected_with, npartitions=1)\n    result = dgpd.sjoin(dask_select_gdf, dask_intersected_gdf, how=join_type, predicate=predicate).compute()\n    result = GeoDataFrame(result)\n    result.sindex  # pylint: disable=W0104\n\n    return result\n</code></pre>"},{"location":"reference/vector/#vector.multiprocessor_spatial_join","title":"multiprocessor_spatial_join","text":"<pre><code>multiprocessor_spatial_join(\n    select_features_from: GeoDataFrame,\n    intersected_with: GeoDataFrame,\n    join_type: str = \"inner\",\n    predicate: str = \"intersects\",\n    num_of_workers: int = 4,\n    logger: Logger = LOGGER,\n) -&gt; GeoDataFrame\n</code></pre> <p>Parameters:</p> Name Type Description Default <code>select_features_from</code> <code>GeoDataFrame</code> <p>Numpy array containing the polygons from which to select features from.</p> required <code>intersected_with</code> <code>GeoDataFrame</code> <p>Geodataframe containing the polygons that will be used to select features with via an intersect operation.</p> required <code>join_type</code> <code>str</code> <p>How the join will be executed. Available join_types are: ['left', 'right', 'inner']. Defaults to 'inner'</p> <code>'inner'</code> <code>predicate</code> <code>str</code> <p>The predicate to use for selecting features from. Available predicates are: ['intersects', 'contains', 'within', 'touches', 'crosses', 'overlaps']. Defaults to 'intersects'</p> <code>'intersects'</code> <code>num_of_workers</code> <code>int</code> <p>The number of processes to use for parallel execution. Defaults to 4.</p> <code>4</code> <code>logger</code> <code>Logger</code> <p>Logger instance.</p> <code>LOGGER</code> <p>Returns:</p> Source code in <code>geospatial_tools/vector.py</code> <pre><code>def multiprocessor_spatial_join(\n    select_features_from: GeoDataFrame,\n    intersected_with: GeoDataFrame,\n    join_type: str = \"inner\",\n    predicate: str = \"intersects\",\n    num_of_workers: int = 4,\n    logger: logging.Logger = LOGGER,\n) -&gt; GeoDataFrame:\n    \"\"\"\n\n    Args:\n      select_features_from: Numpy array containing the polygons from which to select features from.\n      intersected_with: Geodataframe containing the polygons that will be used to select features with via an\n        intersect operation.\n      join_type: How the join will be executed. Available join_types are:\n        ['left', 'right', 'inner']. Defaults to 'inner'\n      predicate: The predicate to use for selecting features from. Available predicates are:\n        ['intersects', 'contains', 'within', 'touches', 'crosses', 'overlaps']. Defaults to 'intersects'\n      num_of_workers: The number of processes to use for parallel execution. Defaults to 4.\n      logger: Logger instance.\n\n    Returns:\n\n\n    \"\"\"\n    select_features_from_chunks = np.array_split(select_features_from, num_of_workers)\n    with ProcessPoolExecutor(max_workers=num_of_workers) as executor:\n        futures = [\n            executor.submit(gpd.sjoin, chunk, intersected_with, how=join_type, predicate=predicate)\n            for chunk in select_features_from_chunks\n        ]\n        intersecting_polygons_list = [future.result() for future in futures]\n    logger.info(\"Concatenating results\")\n    intersecting_polygons = gpd.GeoDataFrame(pd.concat(intersecting_polygons_list, ignore_index=True))\n    logger.info(\"Creating spatial index\")\n    intersecting_polygons.sindex  # pylint: disable=W0104\n    if len(intersected_with) &gt; 1:\n        # This last step is necessary when doing a spatial join where `intersected_with` contains multiple features\n        logger.info(\"Dropping duplicates\")\n        intersecting_polygons = intersecting_polygons.drop_duplicates(subset=\"geometry\")\n    return intersecting_polygons\n</code></pre>"},{"location":"reference/vector/#vector.select_polygons_by_location","title":"select_polygons_by_location","text":"<pre><code>select_polygons_by_location(\n    select_features_from: GeoDataFrame,\n    intersected_with: GeoDataFrame,\n    num_of_workers: int = None,\n    join_type: str = \"inner\",\n    predicate=\"intersects\",\n    join_function=multiprocessor_spatial_join,\n    logger: Logger = LOGGER,\n) -&gt; GeoDataFrame\n</code></pre> <p>This function executes a <code>select by location</code> operation on a GeoDataFrame. It is essentially a wrapper around <code>gpd.sjoin</code> to allow parallel execution. While it does use <code>sjoin</code>, only the columns from <code>select_features_from</code> are kept.</p> <p>Parameters:</p> Name Type Description Default <code>select_features_from</code> <code>GeoDataFrame</code> <p>GeoDataFrame containing the polygons from which to select features from.</p> required <code>intersected_with</code> <code>GeoDataFrame</code> <p>Geodataframe containing the polygons that will be used to select features with via an intersect operation.</p> required <code>num_of_workers</code> <code>int</code> <p>Number of parallel processes to use for execution. If using on a compute cluster, please set a specific amount (ex. 1 per CPU core requested). Defaults to the min of number of CPU cores or number (cpu_count())</p> <code>None</code> <code>join_type</code> <code>str</code> <code>'inner'</code> <code>predicate</code> <p>The predicate to use for selecting features from. Available predicates are: ['intersects', 'contains', 'within', 'touches', 'crosses', 'overlaps']. Defaults to 'intersects'</p> <code>'intersects'</code> <code>join_function</code> <p>Function that will execute the join operation. Available functions are: 'multiprocessor_spatial_join'; 'dask_spatial_join'; or custom functions. (Default value = multiprocessor_spatial_join)</p> <code>multiprocessor_spatial_join</code> <code>logger</code> <code>Logger</code> <p>Logger instance.</p> <code>LOGGER</code> <p>Returns:</p> Source code in <code>geospatial_tools/vector.py</code> <pre><code>def select_polygons_by_location(\n    select_features_from: GeoDataFrame,\n    intersected_with: GeoDataFrame,\n    num_of_workers: int = None,\n    join_type: str = \"inner\",\n    predicate=\"intersects\",\n    join_function=multiprocessor_spatial_join,\n    logger: logging.Logger = LOGGER,\n) -&gt; GeoDataFrame:\n    \"\"\"\n    This function executes a `select by location` operation on a GeoDataFrame. It is essentially a wrapper around\n    `gpd.sjoin` to allow parallel execution. While it does use `sjoin`, only the columns from `select_features_from` are\n    kept.\n\n    Args:\n      select_features_from: GeoDataFrame containing the polygons from which to select features from.\n      intersected_with: Geodataframe containing the polygons that will be used to select features with via an intersect\n        operation.\n      num_of_workers: Number of parallel processes to use for execution. If using\n        on a compute cluster, please set a specific amount (ex. 1 per CPU core requested).\n        Defaults to the min of number of CPU cores\n        or number (cpu_count())\n      join_type:\n      predicate: The predicate to use for selecting features from. Available predicates are:\n        ['intersects', 'contains', 'within', 'touches', 'crosses', 'overlaps']. Defaults to 'intersects'\n      join_function: Function that will execute the join operation. Available functions are:\n        'multiprocessor_spatial_join'; 'dask_spatial_join'; or custom functions.\n        (Default value = multiprocessor_spatial_join)\n      logger: Logger instance.\n\n    Returns:\n    \"\"\"\n    workers = cpu_count()\n    if num_of_workers:\n        workers = num_of_workers\n    logger.info(f\"Number of workers used: {workers}\")\n\n    intersecting_polygons = join_function(\n        select_features_from=select_features_from,\n        intersected_with=intersected_with,\n        join_type=join_type,\n        predicate=predicate,\n        num_of_workers=num_of_workers,\n    )\n    logger.info(\"Filtering columns of the results\")\n    filtered_result_gdf = intersecting_polygons.drop(columns=intersecting_polygons.filter(like=\"_right\").columns)\n    column_list_to_filter = [item for item in intersected_with.columns if item not in select_features_from.columns]\n    conserved_columns = [col for col in filtered_result_gdf.columns if col not in column_list_to_filter]\n    filtered_result_gdf = filtered_result_gdf[conserved_columns]  # pylint: disable=E1136\n\n    return filtered_result_gdf\n</code></pre>"},{"location":"reference/vector/#vector.to_geopackage","title":"to_geopackage","text":"<pre><code>to_geopackage(gdf: GeoDataFrame, filename: str | Path, logger=LOGGER) -&gt; str\n</code></pre> <p>Save GeoDataFrame to a Geopackage file.</p> <p>Parameters:</p> Name Type Description Default <code>gdf</code> <code>GeoDataFrame</code> <p>The GeoDataFrame to save.</p> required <code>filename</code> <code>str | Path</code> <p>The filename to save to.</p> required <code>logger</code> <p>Logger instance (Default value = LOGGER)</p> <code>LOGGER</code> <p>Returns:</p> Source code in <code>geospatial_tools/vector.py</code> <pre><code>def to_geopackage(gdf: GeoDataFrame, filename: str | Path, logger=LOGGER) -&gt; str:\n    \"\"\"\n    Save GeoDataFrame to a Geopackage file.\n\n    Args:\n      gdf: The GeoDataFrame to save.\n      filename: The filename to save to.\n      logger: Logger instance (Default value = LOGGER)\n\n    Returns:\n    \"\"\"\n    start = time.time()\n    logger.info(\"Starting writing process\")\n    if isinstance(gdf, pd.DataFrame):\n        gdf = GeoDataFrame(gdf)\n    gdf.to_file(filename, driver=GEOPACKAGE_DRIVER, mode=\"w\")\n    stop = time.time()\n    logger.info(f\"File [{filename}] took {stop - start} seconds to write.\")\n\n    return filename\n</code></pre>"},{"location":"reference/vector/#vector.to_geopackage_chunked","title":"to_geopackage_chunked","text":"<pre><code>to_geopackage_chunked(\n    gdf: GeoDataFrame, filename: str, chunk_size: int = 1000000, logger: Logger = LOGGER\n) -&gt; str\n</code></pre> <p>Save GeoDataFrame to a Geopackage file using chunks to help with potential memory consumption. This function can potentially be slower than <code>to_geopackage</code>, especially if <code>chunk_size</code> is not adequately defined. Therefore, this function should only be required if <code>to_geopackage</code> fails because of memory issues.</p> <p>Parameters:</p> Name Type Description Default <code>gdf</code> <code>GeoDataFrame</code> <p>The GeoDataFrame to save.</p> required <code>filename</code> <code>str</code> <p>The filename to save to.</p> required <code>chunk_size</code> <code>int</code> <p>The number of rows per chunk.</p> <code>1000000</code> <code>logger</code> <code>Logger</code> <p>Logger instance.</p> <code>LOGGER</code> <p>Returns:</p> Source code in <code>geospatial_tools/vector.py</code> <pre><code>def to_geopackage_chunked(\n    gdf: GeoDataFrame, filename: str, chunk_size: int = 1000000, logger: logging.Logger = LOGGER\n) -&gt; str:\n    \"\"\"\n    Save GeoDataFrame to a Geopackage file using chunks to help with potential memory consumption. This function can\n    potentially be slower than `to_geopackage`, especially if `chunk_size` is not adequately defined. Therefore, this\n    function should only be required if `to_geopackage` fails because of memory issues.\n\n    Args:\n      gdf: The GeoDataFrame to save.\n      filename: The filename to save to.\n      chunk_size: The number of rows per chunk.\n      logger: Logger instance.\n\n    Returns:\n    \"\"\"\n    filename_path = Path(filename)\n    if filename_path.exists():\n        filename_path.unlink()\n\n    start = time.time()\n    logger.info(\"Starting writing process\")\n    logger.info(f\"Chunk size used : [{chunk_size}]\")\n    chunk = gdf.iloc[0:chunk_size]\n    chunk.to_file(filename, driver=GEOPACKAGE_DRIVER, mode=\"w\")\n\n    for i in range(chunk_size, len(gdf), chunk_size):\n        chunk = gdf.iloc[i : i + chunk_size]\n        chunk.to_file(filename, driver=GEOPACKAGE_DRIVER, mode=\"a\")\n\n    stop = time.time()\n    logger.info(f\"File [{filename}] took {stop - start} seconds to write.\")\n\n    return filename\n</code></pre>"},{"location":"reference/vector/#vector.select_all_within_feature","title":"select_all_within_feature","text":"<pre><code>select_all_within_feature(\n    polygon_feature: GeoSeries, vector_features: GeoDataFrame\n) -&gt; GeoSeries\n</code></pre> <p>This function is quite small and simple, but exists mostly as a.</p> <p>Parameters:</p> Name Type Description Default <code>polygon_feature</code> <code>GeoSeries</code> <p>Polygon feature that will be used to find which features of <code>vector_features</code> are contained within it. In this function, it is expected to be a GeoSeries, so a single row from a GeoDataFrame.</p> required <code>vector_features</code> <code>GeoDataFrame</code> <p>The dataframe containing the features that will be grouped by polygon_feature.</p> required <p>Returns:</p> Source code in <code>geospatial_tools/vector.py</code> <pre><code>def select_all_within_feature(polygon_feature: gpd.GeoSeries, vector_features: gpd.GeoDataFrame) -&gt; gpd.GeoSeries:\n    \"\"\"\n    This function is quite small and simple, but exists mostly as a.\n\n    Args:\n      polygon_feature: Polygon feature that will be used to find which features of `vector_features` are contained\n        within it. In this function, it is expected to be a GeoSeries, so a single row from a GeoDataFrame.\n      vector_features: The dataframe containing the features that will be grouped by polygon_feature.\n\n    Returns:\n    \"\"\"\n    contained_features = vector_features[vector_features.within(polygon_feature.geometry)]\n    return contained_features\n</code></pre>"},{"location":"reference/vector/#vector.add_and_fill_contained_column","title":"add_and_fill_contained_column","text":"<pre><code>add_and_fill_contained_column(\n    polygon_feature,\n    polygon_column_name,\n    vector_features,\n    vector_column_name,\n    logger=LOGGER,\n)\n</code></pre> <p>This function make in place changes to <code>vector_geodataframe</code>.</p> <p>The purpose of this function is to first do a spatial search operation on which <code>vector_features</code> are within <code>polygon_feature</code>, and then write the contents found in the <code>polygon_column_name</code> to the selected <code>vector_features</code></p> <p>Parameters:</p> Name Type Description Default <code>polygon_feature</code> <p>Polygon feature that will be used to find which features of <code>vector_features</code> are contained within it.</p> required <code>polygon_column_name</code> <p>The name of the column in <code>polygon_feature</code> that contains the name/id of each polygon to be written to <code>vector_features</code>.</p> required <code>vector_features</code> <p>The dataframe containing the features that will be grouped by polygon_feature.</p> required <code>vector_column_name</code> <p>The name of the column in <code>vector_features</code> that will the name/id of each polygon.</p> required <code>logger</code> <p>Logger instance</p> <code>LOGGER</code> <p>Returns:</p> Source code in <code>geospatial_tools/vector.py</code> <pre><code>def add_and_fill_contained_column(\n    polygon_feature, polygon_column_name, vector_features, vector_column_name, logger=LOGGER\n):\n    \"\"\"\n    This function make in place changes to `vector_geodataframe`.\n\n    The purpose of this function is to first do a spatial search operation on which `vector_features` are within\n    `polygon_feature`, and then write the contents found in the `polygon_column_name` to the selected `vector_features`\n\n    Args:\n      polygon_feature: Polygon feature that will be used to find which features of `vector_features` are contained\n        within it.\n      polygon_column_name: The name of the column in `polygon_feature` that contains the name/id of each polygon to\n        be written to `vector_features`.\n      vector_features: The dataframe containing the features that will be grouped by polygon_feature.\n      vector_column_name: The name of the column in `vector_features` that will the name/id of each polygon.\n      logger: Logger instance\n\n    Returns:\n    \"\"\"\n    feature_name = polygon_feature[polygon_column_name]\n    logger.info(f\"Selecting all vector features that are within {feature_name}\")\n    selected_features = select_all_within_feature(polygon_feature=polygon_feature, vector_features=vector_features)\n    logger.info(f\"Writing [{feature_name}] to selected vector features\")\n\n    vector_features.loc[selected_features.index, vector_column_name] = vector_features.loc[\n        selected_features.index, vector_column_name\n    ].apply(lambda s: s | {feature_name})\n</code></pre>"},{"location":"reference/vector/#vector.find_and_write_all_contained_features","title":"find_and_write_all_contained_features","text":"<pre><code>find_and_write_all_contained_features(\n    polygon_features: GeoDataFrame,\n    polygon_column: str,\n    vector_features: GeoDataFrame,\n    vector_column_name: str,\n    logger=LOGGER,\n)\n</code></pre> <p>This function make in place changes to <code>vector_geodataframe</code>.</p> <p>It iterates on all features of a dataframe containing polygons and executes a spatial search with each polygon to find all vector features from <code>vector_features</code> that are contained by it.</p> <p>The name/id of each polygon is added to a set in a new column in <code>vector_features</code> to identify which features are within which polygon.</p> <p>To make things simple, this is basically a \"group by\" operation based on the \"within\" spatial operator. Each feature in <code>vector_features</code> will have a list of all the polygons that contain it (contain as being completely within the polygon).</p> <p>Parameters:</p> Name Type Description Default <code>polygon_features</code> <code>GeoDataFrame</code> <p>Dataframes containing polygons. Will be used to find which features of <code>vector_features</code> are contained within which polygon</p> required <code>polygon_column</code> <code>str</code> <p>The name of the column in <code>polygon_features</code> that contains the name/id of each polygon.</p> required <code>vector_features</code> <code>GeoDataFrame</code> <p>The dataframe containing the features that will be grouped by polygon.</p> required <code>vector_column_name</code> <code>str</code> <p>The name of the column in <code>vector_features</code> that will the name/id of each polygon.</p> required <code>logger</code> <p>(Default value = LOGGER)</p> <code>LOGGER</code> <p>Returns:</p> Source code in <code>geospatial_tools/vector.py</code> <pre><code>def find_and_write_all_contained_features(\n    polygon_features: gpd.GeoDataFrame,\n    polygon_column: str,\n    vector_features: gpd.GeoDataFrame,\n    vector_column_name: str,\n    logger=LOGGER,\n):\n    \"\"\"\n    This function make in place changes to `vector_geodataframe`.\n\n    It iterates on all features of a dataframe containing polygons and executes a spatial search with each\n    polygon to find all vector features from `vector_features` that are contained by it.\n\n    The name/id of each polygon is added to a set in a new column in\n    `vector_features` to identify which features are within which polygon.\n\n    To make things simple, this is basically a \"group by\" operation based on the\n    \"within\" spatial operator. Each feature in `vector_features` will have a list of\n    all the polygons that contain it (contain as being completely within the polygon).\n\n    Args:\n      polygon_features: Dataframes containing polygons. Will be used to find which features of `vector_features`\n        are contained within which polygon\n      polygon_column: The name of the column in `polygon_features` that contains the name/id\n        of each polygon.\n      vector_features: The dataframe containing the features that will be grouped by polygon.\n      vector_column_name: The name of the column in `vector_features` that will the name/id of each polygon.\n      logger:  (Default value = LOGGER)\n\n    Returns:\n    \"\"\"\n    if vector_column_name not in vector_features.columns:\n        vector_features[vector_column_name] = [set() for _ in range(len(vector_features))]\n\n    logger.info(\"Starting process to find and identify contained features\")\n    polygon_features.apply(\n        lambda row: add_and_fill_contained_column(\n            polygon_feature=row,\n            polygon_column_name=polygon_column,\n            vector_features=vector_features,\n            vector_column_name=vector_column_name,\n        ),\n        axis=1,\n    )\n    vector_features[vector_column_name] = vector_features[vector_column_name].apply(sorted)\n    logger.info(\"Process to find and identify contained features is completed\")\n</code></pre>"},{"location":"reference/vector/#vector.spatial_join_within","title":"spatial_join_within","text":"<pre><code>spatial_join_within(\n    polygon_features: GeoDataFrame,\n    polygon_column: str,\n    vector_features: GeoDataFrame,\n    vector_column_name: str,\n    join_type: str = \"left\",\n    predicate: str = \"within\",\n    logger=LOGGER,\n) -&gt; GeoDataFrame\n</code></pre> <p>This function does approximately the same thing as <code>find_and_write_all_contained_features</code>, but does not make in place changes to <code>vector_features</code> and instead returns a new dataframe.</p> <p>This function is more efficient than <code>find_and_write_all_contained_features</code> but offers less flexibility.</p> <p>It does a spatial join based on a within operation between features to associate which <code>vector_features</code> are within which <code>polygon_features</code>, groups the results by vector feature</p> <p>Parameters:</p> Name Type Description Default <code>polygon_features</code> <code>GeoDataFrame</code> <p>Dataframes containing polygons. Will be used to find which features of <code>vector_features</code> are contained within which polygon</p> required <code>polygon_column</code> <code>str</code> <p>The name of the column in <code>polygon_features</code> that contains the name/id of each polygon.</p> required <code>vector_features</code> <code>GeoDataFrame</code> <p>The dataframe containing the features that will be grouped by polygon.</p> required <code>vector_column_name</code> <code>str</code> <p>The name of the column in <code>vector_features</code> that will contain the name/id of each polygon.</p> required <code>join_type</code> <code>str</code> <code>'left'</code> <code>predicate</code> <code>str</code> <p>The predicate to use for the spatial join operation. Defaults to <code>within</code>.</p> <code>'within'</code> <code>logger</code> <p>Logger instance</p> <code>LOGGER</code> <p>Returns:</p> Source code in <code>geospatial_tools/vector.py</code> <pre><code>def spatial_join_within(\n    polygon_features: gpd.GeoDataFrame,\n    polygon_column: str,\n    vector_features: gpd.GeoDataFrame,\n    vector_column_name: str,\n    join_type: str = \"left\",\n    predicate: str = \"within\",\n    logger=LOGGER,\n) -&gt; gpd.GeoDataFrame:\n    \"\"\"\n    This function does approximately the same thing as `find_and_write_all_contained_features`, but does not make in\n    place changes to `vector_features` and instead returns a new dataframe.\n\n    This function is more efficient than `find_and_write_all_contained_features` but offers less flexibility.\n\n    It does a spatial join based on a within operation between features to associate which `vector_features`\n    are within which `polygon_features`, groups the results by vector feature\n\n    Args:\n      polygon_features: Dataframes containing polygons. Will be used to find which features of `vector_features`\n        are contained within which polygon\n      polygon_column: The name of the column in `polygon_features` that contains the name/id\n        of each polygon.\n      vector_features: The dataframe containing the features that will be grouped by polygon.\n      vector_column_name: The name of the column in `vector_features` that will contain the name/id of each polygon.\n      join_type:\n      predicate: The predicate to use for the spatial join operation. Defaults to `within`.\n      logger: Logger instance\n\n    Returns:\n    \"\"\"\n    temp_feature_id = \"feature_id\"\n    uuid_suffix = str(uuid.uuid4())\n    if temp_feature_id in vector_features.columns:\n        logger.info(\"Creating temporary UUID field for join operations\")\n        temp_feature_id = f\"{temp_feature_id}_{uuid_suffix}\"\n    _generate_uuid_column(df=vector_features, column_name=temp_feature_id)\n    logger.info(\"Starting process to find and identify contained features using spatial 'within' join operation\")\n    joined_gdf = gpd.sjoin(\n        vector_features, polygon_features[[polygon_column, \"geometry\"]], how=join_type, predicate=predicate\n    )\n    logger.info(\"Grouping results\")\n    grouped_gdf = joined_gdf.groupby(temp_feature_id)[polygon_column].agg(list).reset_index()\n    logger.info(\"Cleaning and merging results\")\n    features = vector_features.merge(grouped_gdf, on=temp_feature_id, how=\"left\")\n    features = features.rename(columns={polygon_column: vector_column_name})\n    features.drop(columns=[temp_feature_id], inplace=True)\n    features[vector_column_name] = features[vector_column_name].apply(sorted)\n    logger.info(\"Spatial join operation is completed\")\n    return gpd.GeoDataFrame(features)\n</code></pre>"},{"location":"reference/planetary_computer/","title":"planetary_computer","text":""},{"location":"reference/planetary_computer/#planetary_computer.sentinel_2","title":"sentinel_2","text":""},{"location":"reference/planetary_computer/#planetary_computer.sentinel_2.BestProductsForFeatures","title":"BestProductsForFeatures","text":"<pre><code>BestProductsForFeatures(\n    sentinel2_tiling_grid: GeoDataFrame,\n    sentinel2_tiling_grid_column: str,\n    vector_features: GeoDataFrame,\n    vector_features_column: str,\n    date_ranges: list[str] | None = None,\n    max_cloud_cover: int = 5,\n    max_no_data_value: int = 5,\n    logger: Logger = LOGGER,\n)\n</code></pre> <p>Class made to facilitate and automate searching for Sentinel 2 products using the Sentinel 2 tiling grid as a reference.</p> <p>Current limitation is that vector features used must fit, or be completely contained inside a single Sentinel 2 tiling grid.</p> <p>For larger features, a mosaic of products will be necessary.</p> <p>This class was conceived first and foremost to be used for numerous smaller vector features, like polygon grids created from <code>geospatial_tools.vector.create_vector_grid</code></p> <p>Parameters:</p> Name Type Description Default <code>sentinel2_tiling_grid</code> <code>GeoDataFrame</code> <p>GeoDataFrame containing Sentinel 2 tiling grid</p> required <code>sentinel2_tiling_grid_column</code> <code>str</code> <p>Name of the column in <code>sentinel2_tiling_grid</code> that contains the tile names (ex tile name: 10SDJ)</p> required <code>vector_features</code> <code>GeoDataFrame</code> <p>GeoDataFrame containing the vector features for which the best Sentinel 2 products will be chosen for.</p> required <code>vector_features_column</code> <code>str</code> <p>Name of the column in <code>vector_features</code> where the best Sentinel 2 products will be written to</p> required <code>date_ranges</code> <code>list[str] | None</code> <p>Date range used to search for Sentinel 2 products. should be created using <code>geospatial_tools.utils.create_date_range_for_specific_period</code> separately, or <code>BestProductsForFeatures.create_date_range</code> after initialization.</p> <code>None</code> <code>max_cloud_cover</code> <code>int</code> <p>Maximum cloud cover used to search for Sentinel 2 products.</p> <code>5</code> <code>logger</code> <code>Logger</code> <p>Logger instance</p> <code>LOGGER</code> Source code in <code>geospatial_tools/planetary_computer/sentinel_2.py</code> <pre><code>def __init__(\n    self,\n    sentinel2_tiling_grid: GeoDataFrame,\n    sentinel2_tiling_grid_column: str,\n    vector_features: GeoDataFrame,\n    vector_features_column: str,\n    date_ranges: list[str] | None = None,\n    max_cloud_cover: int = 5,\n    max_no_data_value: int = 5,\n    logger: logging.Logger = LOGGER,\n):\n    \"\"\"\n\n    Args:\n        sentinel2_tiling_grid: GeoDataFrame containing Sentinel 2 tiling grid\n        sentinel2_tiling_grid_column: Name of the column in `sentinel2_tiling_grid` that contains the tile names\n            (ex tile name: 10SDJ)\n        vector_features: GeoDataFrame containing the vector features for which the best Sentinel 2\n            products will be chosen for.\n        vector_features_column: Name of the column in `vector_features` where the best Sentinel 2 products\n            will be written to\n        date_ranges: Date range used to search for Sentinel 2 products. should be created using\n            `geospatial_tools.utils.create_date_range_for_specific_period` separately,\n            or `BestProductsForFeatures.create_date_range` after initialization.\n        max_cloud_cover: Maximum cloud cover used to search for Sentinel 2 products.\n        logger: Logger instance\n    \"\"\"\n    self.logger = logger\n    self.sentinel2_tiling_grid = sentinel2_tiling_grid\n    self.sentinel2_tiling_grid_column = sentinel2_tiling_grid_column\n    self.sentinel2_tile_list = sentinel2_tiling_grid[\"name\"].to_list()\n    self.vector_features = vector_features\n    self.vector_features_column = vector_features_column\n    self.vector_features_best_product_column = \"best_s2_product_id\"\n    self.vector_features_with_products = None\n    self._date_ranges = date_ranges\n    self._max_cloud_cover = max_cloud_cover\n    self.max_no_data_value = max_no_data_value\n    self.successful_results = {}\n    self.incomplete_results = []\n    self.error_results = []\n</code></pre>"},{"location":"reference/planetary_computer/#planetary_computer.sentinel_2.BestProductsForFeatures.max_cloud_cover","title":"max_cloud_cover  <code>property</code> <code>writable</code>","text":"<pre><code>max_cloud_cover\n</code></pre> <p>Max % of cloud cover used for Sentinel 2 product search.</p>"},{"location":"reference/planetary_computer/#planetary_computer.sentinel_2.BestProductsForFeatures.date_ranges","title":"date_ranges  <code>property</code> <code>writable</code>","text":"<pre><code>date_ranges\n</code></pre> <p>Date range used to search for Sentinel 2 products.</p>"},{"location":"reference/planetary_computer/#planetary_computer.sentinel_2.BestProductsForFeatures.create_date_ranges","title":"create_date_ranges","text":"<pre><code>create_date_ranges(\n    start_year: int, end_year: int, start_month: int, end_month: int\n) -&gt; list[str]\n</code></pre> <p>This function create a list of date ranges.</p> <p>For example, I want to create date ranges for 2020 and 2021, but only for the months from March to May. I therefore expect to have 2 ranges: [2020-03-01 to 2020-05-30, 2021-03-01 to 2021-05-30].</p> <p>Handles the automatic definition of the last day for the end month, as well as periods that cross over years</p> <p>For example, I want to create date ranges for 2020 and 2022, but only for the months from November to January. I therefore expect to have 2 ranges: [2020-11-01 to 2021-01-31, 2021-11-01 to 2022-01-31].</p> <p>Parameters:</p> Name Type Description Default <code>start_year</code> <code>int</code> <p>Start year for ranges</p> required <code>end_year</code> <code>int</code> <p>End year for ranges</p> required <code>start_month</code> <code>int</code> <p>Starting month for each period</p> required <code>end_month</code> <code>int</code> <p>End month for each period (inclusively)</p> required <p>Returns:</p> Type Description <code>list[str]</code> <p>List of date ranges</p> Source code in <code>geospatial_tools/planetary_computer/sentinel_2.py</code> <pre><code>def create_date_ranges(self, start_year: int, end_year: int, start_month: int, end_month: int) -&gt; list[str]:\n    \"\"\"\n    This function create a list of date ranges.\n\n    For example, I want to create date ranges for 2020 and 2021, but only for the months from March to May.\n    I therefore expect to have 2 ranges: [2020-03-01 to 2020-05-30, 2021-03-01 to 2021-05-30].\n\n    Handles the automatic definition of the last day for the end month, as well as periods that cross over years\n\n    For example, I want to create date ranges for 2020 and 2022, but only for the months from November to January.\n    I therefore expect to have 2 ranges: [2020-11-01 to 2021-01-31, 2021-11-01 to 2022-01-31].\n\n    Args:\n      start_year: Start year for ranges\n      end_year: End year for ranges\n      start_month: Starting month for each period\n      end_month: End month for each period (inclusively)\n\n    Returns:\n        List of date ranges\n    \"\"\"\n    self.date_ranges = create_date_range_for_specific_period(\n        start_year=start_year, end_year=end_year, start_month_range=start_month, end_month_range=end_month\n    )\n    return self.date_ranges\n</code></pre>"},{"location":"reference/planetary_computer/#planetary_computer.sentinel_2.BestProductsForFeatures.find_best_complete_products","title":"find_best_complete_products","text":"<pre><code>find_best_complete_products(\n    max_cloud_cover: int | None = None, max_no_data_value: int = 5\n) -&gt; dict\n</code></pre> <p>Finds the best complete products for each Sentinel 2 tiles. This function will filter out all products that have more than 5% of nodata values.</p> <p>Filtered out tiles will be stored in <code>self.incomplete</code> and tiles for which the search has found no results will be stored in <code>self.error_list</code></p> <p>Parameters:</p> Name Type Description Default <code>max_cloud_cover</code> <code>int | None</code> <p>Max percentage of cloud cover allowed used for the search  (Default value = None)</p> <code>None</code> <code>max_no_data_value</code> <code>int</code> <p>Max percentage of no-data coverage by individual Sentinel 2 product  (Default value = 5)</p> <code>5</code> <p>Returns:</p> Type Description <code>dict</code> <p>Dictionary of product IDs and their corresponding Sentinel 2 tile names.</p> Source code in <code>geospatial_tools/planetary_computer/sentinel_2.py</code> <pre><code>def find_best_complete_products(self, max_cloud_cover: int | None = None, max_no_data_value: int = 5) -&gt; dict:\n    \"\"\"\n    Finds the best complete products for each Sentinel 2 tiles. This function will filter out all products that have\n    more than 5% of nodata values.\n\n    Filtered out tiles will be stored in `self.incomplete` and tiles for which\n    the search has found no results will be stored in `self.error_list`\n\n    Args:\n      max_cloud_cover: Max percentage of cloud cover allowed used for the search  (Default value = None)\n      max_no_data_value: Max percentage of no-data coverage by individual Sentinel 2 product  (Default value = 5)\n\n    Returns:\n        Dictionary of product IDs and their corresponding Sentinel 2 tile names.\n    \"\"\"\n    cloud_cover = self.max_cloud_cover\n    if max_cloud_cover:\n        cloud_cover = max_cloud_cover\n    no_data_value = self.max_no_data_value\n    if max_no_data_value:\n        no_data_value = max_no_data_value\n\n    tile_dict, incomplete_list, error_list = find_best_product_per_s2_tile(\n        date_ranges=self.date_ranges,\n        max_cloud_cover=cloud_cover,\n        s2_tile_grid_list=self.sentinel2_tile_list,\n        num_of_workers=4,\n        max_no_data_value=no_data_value,\n    )\n    self.successful_results = tile_dict\n    self.incomplete_results = incomplete_list\n    if incomplete_list:\n        self.logger.warning(\n            \"Warning, some of the input Sentinel 2 tiles do not have products covering the entire tile. \"\n            \"These tiles will need to be handled differently (ex. creating a mosaic with multiple products\"\n        )\n        self.logger.warning(f\"Incomplete list: {incomplete_list}\")\n    self.error_results = error_list\n    if error_list:\n        self.logger.warning(\n            \"Warning, products for some Sentinel 2 tiles could not be found. \"\n            \"Consider either extending date range input or max cloud cover\"\n        )\n        self.logger.warning(f\"Error list: {error_list}\")\n    return self.successful_results\n</code></pre>"},{"location":"reference/planetary_computer/#planetary_computer.sentinel_2.BestProductsForFeatures.select_best_products_per_feature","title":"select_best_products_per_feature","text":"<pre><code>select_best_products_per_feature() -&gt; GeoDataFrame\n</code></pre> <p>Return a GeoDataFrame containing the best products for each Sentinel 2 tile.</p> Source code in <code>geospatial_tools/planetary_computer/sentinel_2.py</code> <pre><code>def select_best_products_per_feature(self) -&gt; GeoDataFrame:\n    \"\"\"Return a GeoDataFrame containing the best products for each Sentinel 2 tile.\"\"\"\n    spatial_join_results = spatial_join_within(\n        polygon_features=self.sentinel2_tiling_grid,\n        polygon_column=self.sentinel2_tiling_grid_column,\n        vector_features=self.vector_features,\n        vector_column_name=self.vector_features_column,\n    )\n    write_best_product_ids_to_dataframe(\n        spatial_join_results=spatial_join_results,\n        tile_dictionary=self.successful_results,\n        best_product_column=self.vector_features_best_product_column,\n        s2_tiles_column=self.vector_features_column,\n    )\n    self.vector_features_with_products = spatial_join_results\n    return self.vector_features_with_products\n</code></pre>"},{"location":"reference/planetary_computer/#planetary_computer.sentinel_2.BestProductsForFeatures.to_file","title":"to_file","text":"<pre><code>to_file(output_dir: str | Path) -&gt; None\n</code></pre> <p>Parameters:</p> Name Type Description Default <code>output_dir</code> <code>str | Path</code> <p>Output directory used to write to file</p> required Source code in <code>geospatial_tools/planetary_computer/sentinel_2.py</code> <pre><code>def to_file(self, output_dir: str | pathlib.Path) -&gt; None:\n    \"\"\"\n\n    Args:\n      output_dir: Output directory used to write to file\n    \"\"\"\n    write_results_to_file(\n        cloud_cover=self.max_cloud_cover,\n        successful_results=self.successful_results,\n        incomplete_results=self.incomplete_results,\n        error_results=self.error_results,\n        output_dir=output_dir,\n    )\n</code></pre>"},{"location":"reference/planetary_computer/#planetary_computer.sentinel_2.sentinel_2_complete_tile_search","title":"sentinel_2_complete_tile_search","text":"<pre><code>sentinel_2_complete_tile_search(\n    tile_id: int,\n    date_ranges: list[str],\n    max_cloud_cover: int,\n    max_no_data_value: int = 5,\n) -&gt; tuple[int, str, float | None, float | None] | None\n</code></pre> <p>Parameters:</p> Name Type Description Default <code>tile_id</code> <code>int</code> required <code>date_ranges</code> <code>list[str]</code> required <code>max_cloud_cover</code> <code>int</code> required <code>max_no_data_value</code> <code>int</code> <p>(Default value = 5)</p> <code>5</code> <p>Returns:</p> Source code in <code>geospatial_tools/planetary_computer/sentinel_2.py</code> <pre><code>def sentinel_2_complete_tile_search(\n    tile_id: int,\n    date_ranges: list[str],\n    max_cloud_cover: int,\n    max_no_data_value: int = 5,\n) -&gt; tuple[int, str, float | None, float | None] | None:\n    \"\"\"\n\n    Args:\n      tile_id:\n      date_ranges:\n      max_cloud_cover:\n      max_no_data_value: (Default value = 5)\n\n    Returns:\n\n\n    \"\"\"\n    client = StacSearch(PLANETARY_COMPUTER)\n    collection = \"sentinel-2-l2a\"\n    tile_ids = [tile_id]\n    query = {\"eo:cloud_cover\": {\"lt\": max_cloud_cover}, \"s2:mgrs_tile\": {\"in\": tile_ids}}\n    sortby = [{\"field\": \"properties.eo:cloud_cover\", \"direction\": \"asc\"}]\n\n    client.search_for_date_ranges(\n        date_ranges=date_ranges, collections=collection, query=query, sortby=sortby, limit=100\n    )\n    try:\n        sorted_items = client.sort_results_by_cloud_coverage()\n        if not sorted_items:\n            return tile_id, \"error: No results found\", None, None\n        filtered_items = client.filter_no_data(\n            property_name=\"s2:nodata_pixel_percentage\", max_no_data_value=max_no_data_value\n        )\n        if not filtered_items:\n            return tile_id, \"incomplete: No results found that cover the entire tile\", None, None\n        optimal_result = filtered_items[0]\n        if optimal_result:\n            return (\n                tile_id,\n                optimal_result.id,\n                optimal_result.properties[\"eo:cloud_cover\"],\n                optimal_result.properties[\"s2:nodata_pixel_percentage\"],\n            )\n\n    except (IndexError, TypeError) as error:\n        print(error)\n        return tile_id, f\"error: {error}\", None, None\n</code></pre>"},{"location":"reference/planetary_computer/#planetary_computer.sentinel_2.find_best_product_per_s2_tile","title":"find_best_product_per_s2_tile","text":"<pre><code>find_best_product_per_s2_tile(\n    date_ranges: list[str],\n    max_cloud_cover: int,\n    s2_tile_grid_list: list,\n    max_no_data_value: int = 5,\n    num_of_workers: int = 4,\n)\n</code></pre> <p>Parameters:</p> Name Type Description Default <code>date_ranges</code> <code>list[str]</code> required <code>max_cloud_cover</code> <code>int</code> required <code>s2_tile_grid_list</code> <code>list</code> required <code>max_no_data_value</code> <code>int</code> <p>(Default value = 5)</p> <code>5</code> <code>num_of_workers</code> <code>int</code> <p>(Default value = 4)</p> <code>4</code> <p>Returns:</p> Source code in <code>geospatial_tools/planetary_computer/sentinel_2.py</code> <pre><code>def find_best_product_per_s2_tile(\n    date_ranges: list[str],\n    max_cloud_cover: int,\n    s2_tile_grid_list: list,\n    max_no_data_value: int = 5,\n    num_of_workers: int = 4,\n):\n    \"\"\"\n\n    Args:\n      date_ranges:\n      max_cloud_cover:\n      s2_tile_grid_list:\n      max_no_data_value:  (Default value = 5)\n      num_of_workers: (Default value = 4)\n\n    Returns:\n\n\n    \"\"\"\n    successful_results = {}\n    for tile in s2_tile_grid_list:\n        successful_results[tile] = \"\"\n    incomplete_results = []\n    error_results = []\n    with ThreadPoolExecutor(max_workers=num_of_workers) as executor:\n        future_to_tile = {\n            executor.submit(\n                sentinel_2_complete_tile_search,\n                tile_id=tile,\n                date_ranges=date_ranges,\n                max_cloud_cover=max_cloud_cover,\n                max_no_data_value=max_no_data_value,\n            ): tile\n            for tile in s2_tile_grid_list\n        }\n\n        for future in as_completed(future_to_tile):\n            tile_id, optimal_result_id, max_cloud_cover, no_data = future.result()\n            if optimal_result_id.startswith(\"error:\"):\n                error_results.append(tile_id)\n                continue\n            if optimal_result_id.startswith(\"incomplete:\"):\n                incomplete_results.append(tile_id)\n                continue\n            successful_results[tile_id] = {\"id\": optimal_result_id, \"cloud_cover\": max_cloud_cover, \"no_data\": no_data}\n        cleaned_successful_results = {k: v for k, v in successful_results.items() if v != \"\"}\n    return cleaned_successful_results, incomplete_results, error_results\n</code></pre>"},{"location":"reference/planetary_computer/#planetary_computer.sentinel_2.write_best_product_ids_to_dataframe","title":"write_best_product_ids_to_dataframe","text":"<pre><code>write_best_product_ids_to_dataframe(\n    spatial_join_results: GeoDataFrame,\n    tile_dictionary: dict,\n    best_product_column: str = \"best_s2_product_id\",\n    s2_tiles_column: str = \"s2_tiles\",\n    logger: Logger = LOGGER,\n)\n</code></pre> <p>Parameters:</p> Name Type Description Default <code>spatial_join_results</code> <code>GeoDataFrame</code> required <code>tile_dictionary</code> <code>dict</code> required <code>best_product_column</code> <code>str</code> <code>'best_s2_product_id'</code> <code>s2_tiles_column</code> <code>str</code> <code>'s2_tiles'</code> <code>logger</code> <code>Logger</code> <code>LOGGER</code> <p>Returns:</p> Source code in <code>geospatial_tools/planetary_computer/sentinel_2.py</code> <pre><code>def write_best_product_ids_to_dataframe(\n    spatial_join_results: GeoDataFrame,\n    tile_dictionary: dict,\n    best_product_column: str = \"best_s2_product_id\",\n    s2_tiles_column: str = \"s2_tiles\",\n    logger: logging.Logger = LOGGER,\n):\n    \"\"\"\n\n    Args:\n      spatial_join_results:\n      tile_dictionary:\n      best_product_column:\n      s2_tiles_column:\n      logger:\n\n    Returns:\n\n\n    \"\"\"\n    logger.info(\"Writing best product IDs to dataframe\")\n    spatial_join_results[best_product_column] = spatial_join_results[s2_tiles_column].apply(\n        lambda x: _get_best_product_id_for_each_grid_tile(s2_tile_search_results=tile_dictionary, feature_s2_tiles=x)\n    )\n</code></pre>"},{"location":"reference/planetary_computer/#planetary_computer.sentinel_2.write_results_to_file","title":"write_results_to_file","text":"<pre><code>write_results_to_file(\n    cloud_cover: int,\n    successful_results: dict,\n    incomplete_results: list | None = None,\n    error_results: list | None = None,\n    output_dir: str | Path = DATA_DIR,\n    logger: Logger = LOGGER,\n) -&gt; dict\n</code></pre> <p>Parameters:</p> Name Type Description Default <code>cloud_cover</code> <code>int</code> required <code>successful_results</code> <code>dict</code> required <code>incomplete_results</code> <code>list | None</code> <code>None</code> <code>error_results</code> <code>list | None</code> <code>None</code> <code>output_dir</code> <code>str | Path</code> <code>DATA_DIR</code> <code>logger</code> <code>Logger</code> <code>LOGGER</code> <p>Returns:</p> Source code in <code>geospatial_tools/planetary_computer/sentinel_2.py</code> <pre><code>def write_results_to_file(\n    cloud_cover: int,\n    successful_results: dict,\n    incomplete_results: list | None = None,\n    error_results: list | None = None,\n    output_dir: str | pathlib.Path = DATA_DIR,\n    logger: logging.Logger = LOGGER,\n) -&gt; dict:\n    \"\"\"\n\n    Args:\n      cloud_cover:\n      successful_results:\n      incomplete_results:\n      error_results:\n      output_dir:\n      logger:\n\n    Returns:\n\n\n    \"\"\"\n    tile_filename = output_dir / f\"data_lt{cloud_cover}cc.json\"\n    with open(tile_filename, \"w\", encoding=\"utf-8\") as json_file:\n        json.dump(successful_results, json_file, indent=4)\n    logger.info(f\"Results have been written to {tile_filename}\")\n\n    incomplete_filename = \"None\"\n    if incomplete_results:\n        incomplete_dict = {\"incomplete\": incomplete_results}\n        incomplete_filename = output_dir / f\"incomplete_lt{cloud_cover}cc.json\"\n        with open(incomplete_filename, \"w\", encoding=\"utf-8\") as json_file:\n            json.dump(incomplete_dict, json_file, indent=4)\n        logger.info(f\"Incomplete results have been written to {incomplete_filename}\")\n\n    error_filename = \"None\"\n    if error_results:\n        error_dict = {\"errors\": error_results}\n        error_filename = output_dir / f\"errors_lt{cloud_cover}cc.json\"\n        with open(error_filename, \"w\", encoding=\"utf-8\") as json_file:\n            json.dump(error_dict, json_file, indent=4)\n        logger.info(f\"Errors results have been written to {error_filename}\")\n\n    return {\n        \"tile_filename\": tile_filename,\n        \"incomplete_filename\": incomplete_filename,\n        \"errors_filename\": error_filename,\n    }\n</code></pre>"},{"location":"reference/planetary_computer/#planetary_computer.sentinel_2.download_and_process_sentinel2_asset","title":"download_and_process_sentinel2_asset","text":"<pre><code>download_and_process_sentinel2_asset(\n    product_id: str,\n    product_bands: list[str],\n    collections: str = \"sentinel-2-l2a\",\n    target_projection: int | str | None = None,\n    base_directory: str | Path = DATA_DIR,\n    delete_intermediate_files: bool = False,\n    logger: Logger = LOGGER,\n) -&gt; Asset\n</code></pre> <p>This function downloads a Sentinel 2 product based on the product ID provided.</p> <p>It will download the individual asset bands provided in the <code>bands</code> argument, merge then all in a single tif and then reproject them to the input CRS.</p> <p>Parameters:</p> Name Type Description Default <code>product_id</code> <code>str</code> <p>ID of the Sentinel 2 product to be downloaded</p> required <code>product_bands</code> <code>list[str]</code> <p>List of the product bands to be downloaded</p> required <code>collections</code> <code>str</code> <p>Collections to be downloaded from. Defaults to <code>sentinel-2-l2a</code></p> <code>'sentinel-2-l2a'</code> <code>target_projection</code> <code>int | str | None</code> <p>The CRS project for the end product. If <code>None</code>, the reprojection step will be skipped</p> <code>None</code> <code>stac_client</code> <p>StacSearch client to used. A new one will be created if not provided</p> required <code>base_directory</code> <code>str | Path</code> <p>The base directory path where the downloaded files will be stored</p> <code>DATA_DIR</code> <code>delete_intermediate_files</code> <code>bool</code> <p>Flag to determine if intermediate files should be deleted. Defaults to False</p> <code>False</code> <code>logger</code> <code>Logger</code> <p>Logger instance</p> <code>LOGGER</code> <p>Returns:</p> Type Description <code>Asset</code> <p>Asset instance</p> Source code in <code>geospatial_tools/planetary_computer/sentinel_2.py</code> <pre><code>def download_and_process_sentinel2_asset(\n    product_id: str,\n    product_bands: list[str],\n    collections: str = \"sentinel-2-l2a\",\n    target_projection: int | str | None = None,\n    base_directory: str | pathlib.Path = DATA_DIR,\n    delete_intermediate_files: bool = False,\n    logger: logging.Logger = LOGGER,\n) -&gt; Asset:\n    \"\"\"\n    This function downloads a Sentinel 2 product based on the product ID provided.\n\n    It will download the individual asset bands provided in the `bands` argument,\n    merge then all in a single tif and then reproject them to the input CRS.\n\n    Args:\n      product_id: ID of the Sentinel 2 product to be downloaded\n      product_bands: List of the product bands to be downloaded\n      collections: Collections to be downloaded from. Defaults to `sentinel-2-l2a`\n      target_projection: The CRS project for the end product. If `None`, the reprojection step will be\n        skipped\n      stac_client: StacSearch client to used. A new one will be created if not provided\n      base_directory: The base directory path where the downloaded files will be stored\n      delete_intermediate_files: Flag to determine if intermediate files should be deleted. Defaults to False\n      logger: Logger instance\n\n    Returns:\n        Asset instance\n    \"\"\"\n    base_file_name = f\"{base_directory}/{product_id}\"\n    merged_file = f\"{base_file_name}_merged.tif\"\n    reprojected_file = f\"{base_file_name}_reprojected.tif\"\n\n    merged_file_exists = pathlib.Path(merged_file).exists()\n    reprojected_file_exists = pathlib.Path(reprojected_file).exists()\n\n    if reprojected_file_exists:\n        logger.info(f\"Reprojected file [{reprojected_file}] already exists\")\n        asset = Asset(asset_id=product_id, bands=product_bands, reprojected_asset=reprojected_file)\n        return asset\n\n    if merged_file_exists:\n        logger.info(f\"Merged file [{merged_file}] already exists\")\n        asset = Asset(asset_id=product_id, bands=product_bands, merged_asset_path=merged_file)\n        if target_projection:\n            logger.info(f\"Reprojecting merged file [{merged_file}]\")\n            asset.reproject_merged_asset(\n                base_directory=base_directory,\n                target_projection=target_projection,\n                delete_merged_asset=delete_intermediate_files,\n            )\n        return asset\n\n    stac_client = StacSearch(catalog_name=PLANETARY_COMPUTER)\n    items = stac_client.search(collections=collections, ids=[product_id])\n    logger.info(items)\n    asset_list = stac_client.download_search_results(bands=product_bands, base_directory=base_directory)\n    logger.info(asset_list)\n    asset = asset_list[0]\n    asset.merge_asset(base_directory=base_directory, delete_sub_items=delete_intermediate_files)\n    if not target_projection:\n        logger.info(\"Skipping reprojection\")\n        return asset\n    if target_projection:\n        asset.reproject_merged_asset(\n            target_projection=target_projection,\n            base_directory=base_directory,\n            delete_merged_asset=delete_intermediate_files,\n        )\n    return asset\n</code></pre>"},{"location":"reference/planetary_computer/sentinel_2/","title":"sentinel_2","text":""},{"location":"reference/planetary_computer/sentinel_2/#planetary_computer.sentinel_2.BestProductsForFeatures","title":"BestProductsForFeatures","text":"<pre><code>BestProductsForFeatures(\n    sentinel2_tiling_grid: GeoDataFrame,\n    sentinel2_tiling_grid_column: str,\n    vector_features: GeoDataFrame,\n    vector_features_column: str,\n    date_ranges: list[str] | None = None,\n    max_cloud_cover: int = 5,\n    max_no_data_value: int = 5,\n    logger: Logger = LOGGER,\n)\n</code></pre> <p>Class made to facilitate and automate searching for Sentinel 2 products using the Sentinel 2 tiling grid as a reference.</p> <p>Current limitation is that vector features used must fit, or be completely contained inside a single Sentinel 2 tiling grid.</p> <p>For larger features, a mosaic of products will be necessary.</p> <p>This class was conceived first and foremost to be used for numerous smaller vector features, like polygon grids created from <code>geospatial_tools.vector.create_vector_grid</code></p> <p>Parameters:</p> Name Type Description Default <code>sentinel2_tiling_grid</code> <code>GeoDataFrame</code> <p>GeoDataFrame containing Sentinel 2 tiling grid</p> required <code>sentinel2_tiling_grid_column</code> <code>str</code> <p>Name of the column in <code>sentinel2_tiling_grid</code> that contains the tile names (ex tile name: 10SDJ)</p> required <code>vector_features</code> <code>GeoDataFrame</code> <p>GeoDataFrame containing the vector features for which the best Sentinel 2 products will be chosen for.</p> required <code>vector_features_column</code> <code>str</code> <p>Name of the column in <code>vector_features</code> where the best Sentinel 2 products will be written to</p> required <code>date_ranges</code> <code>list[str] | None</code> <p>Date range used to search for Sentinel 2 products. should be created using <code>geospatial_tools.utils.create_date_range_for_specific_period</code> separately, or <code>BestProductsForFeatures.create_date_range</code> after initialization.</p> <code>None</code> <code>max_cloud_cover</code> <code>int</code> <p>Maximum cloud cover used to search for Sentinel 2 products.</p> <code>5</code> <code>logger</code> <code>Logger</code> <p>Logger instance</p> <code>LOGGER</code> Source code in <code>geospatial_tools/planetary_computer/sentinel_2.py</code> <pre><code>def __init__(\n    self,\n    sentinel2_tiling_grid: GeoDataFrame,\n    sentinel2_tiling_grid_column: str,\n    vector_features: GeoDataFrame,\n    vector_features_column: str,\n    date_ranges: list[str] | None = None,\n    max_cloud_cover: int = 5,\n    max_no_data_value: int = 5,\n    logger: logging.Logger = LOGGER,\n):\n    \"\"\"\n\n    Args:\n        sentinel2_tiling_grid: GeoDataFrame containing Sentinel 2 tiling grid\n        sentinel2_tiling_grid_column: Name of the column in `sentinel2_tiling_grid` that contains the tile names\n            (ex tile name: 10SDJ)\n        vector_features: GeoDataFrame containing the vector features for which the best Sentinel 2\n            products will be chosen for.\n        vector_features_column: Name of the column in `vector_features` where the best Sentinel 2 products\n            will be written to\n        date_ranges: Date range used to search for Sentinel 2 products. should be created using\n            `geospatial_tools.utils.create_date_range_for_specific_period` separately,\n            or `BestProductsForFeatures.create_date_range` after initialization.\n        max_cloud_cover: Maximum cloud cover used to search for Sentinel 2 products.\n        logger: Logger instance\n    \"\"\"\n    self.logger = logger\n    self.sentinel2_tiling_grid = sentinel2_tiling_grid\n    self.sentinel2_tiling_grid_column = sentinel2_tiling_grid_column\n    self.sentinel2_tile_list = sentinel2_tiling_grid[\"name\"].to_list()\n    self.vector_features = vector_features\n    self.vector_features_column = vector_features_column\n    self.vector_features_best_product_column = \"best_s2_product_id\"\n    self.vector_features_with_products = None\n    self._date_ranges = date_ranges\n    self._max_cloud_cover = max_cloud_cover\n    self.max_no_data_value = max_no_data_value\n    self.successful_results = {}\n    self.incomplete_results = []\n    self.error_results = []\n</code></pre>"},{"location":"reference/planetary_computer/sentinel_2/#planetary_computer.sentinel_2.BestProductsForFeatures.max_cloud_cover","title":"max_cloud_cover  <code>property</code> <code>writable</code>","text":"<pre><code>max_cloud_cover\n</code></pre> <p>Max % of cloud cover used for Sentinel 2 product search.</p>"},{"location":"reference/planetary_computer/sentinel_2/#planetary_computer.sentinel_2.BestProductsForFeatures.date_ranges","title":"date_ranges  <code>property</code> <code>writable</code>","text":"<pre><code>date_ranges\n</code></pre> <p>Date range used to search for Sentinel 2 products.</p>"},{"location":"reference/planetary_computer/sentinel_2/#planetary_computer.sentinel_2.BestProductsForFeatures.create_date_ranges","title":"create_date_ranges","text":"<pre><code>create_date_ranges(\n    start_year: int, end_year: int, start_month: int, end_month: int\n) -&gt; list[str]\n</code></pre> <p>This function create a list of date ranges.</p> <p>For example, I want to create date ranges for 2020 and 2021, but only for the months from March to May. I therefore expect to have 2 ranges: [2020-03-01 to 2020-05-30, 2021-03-01 to 2021-05-30].</p> <p>Handles the automatic definition of the last day for the end month, as well as periods that cross over years</p> <p>For example, I want to create date ranges for 2020 and 2022, but only for the months from November to January. I therefore expect to have 2 ranges: [2020-11-01 to 2021-01-31, 2021-11-01 to 2022-01-31].</p> <p>Parameters:</p> Name Type Description Default <code>start_year</code> <code>int</code> <p>Start year for ranges</p> required <code>end_year</code> <code>int</code> <p>End year for ranges</p> required <code>start_month</code> <code>int</code> <p>Starting month for each period</p> required <code>end_month</code> <code>int</code> <p>End month for each period (inclusively)</p> required <p>Returns:</p> Type Description <code>list[str]</code> <p>List of date ranges</p> Source code in <code>geospatial_tools/planetary_computer/sentinel_2.py</code> <pre><code>def create_date_ranges(self, start_year: int, end_year: int, start_month: int, end_month: int) -&gt; list[str]:\n    \"\"\"\n    This function create a list of date ranges.\n\n    For example, I want to create date ranges for 2020 and 2021, but only for the months from March to May.\n    I therefore expect to have 2 ranges: [2020-03-01 to 2020-05-30, 2021-03-01 to 2021-05-30].\n\n    Handles the automatic definition of the last day for the end month, as well as periods that cross over years\n\n    For example, I want to create date ranges for 2020 and 2022, but only for the months from November to January.\n    I therefore expect to have 2 ranges: [2020-11-01 to 2021-01-31, 2021-11-01 to 2022-01-31].\n\n    Args:\n      start_year: Start year for ranges\n      end_year: End year for ranges\n      start_month: Starting month for each period\n      end_month: End month for each period (inclusively)\n\n    Returns:\n        List of date ranges\n    \"\"\"\n    self.date_ranges = create_date_range_for_specific_period(\n        start_year=start_year, end_year=end_year, start_month_range=start_month, end_month_range=end_month\n    )\n    return self.date_ranges\n</code></pre>"},{"location":"reference/planetary_computer/sentinel_2/#planetary_computer.sentinel_2.BestProductsForFeatures.find_best_complete_products","title":"find_best_complete_products","text":"<pre><code>find_best_complete_products(\n    max_cloud_cover: int | None = None, max_no_data_value: int = 5\n) -&gt; dict\n</code></pre> <p>Finds the best complete products for each Sentinel 2 tiles. This function will filter out all products that have more than 5% of nodata values.</p> <p>Filtered out tiles will be stored in <code>self.incomplete</code> and tiles for which the search has found no results will be stored in <code>self.error_list</code></p> <p>Parameters:</p> Name Type Description Default <code>max_cloud_cover</code> <code>int | None</code> <p>Max percentage of cloud cover allowed used for the search  (Default value = None)</p> <code>None</code> <code>max_no_data_value</code> <code>int</code> <p>Max percentage of no-data coverage by individual Sentinel 2 product  (Default value = 5)</p> <code>5</code> <p>Returns:</p> Type Description <code>dict</code> <p>Dictionary of product IDs and their corresponding Sentinel 2 tile names.</p> Source code in <code>geospatial_tools/planetary_computer/sentinel_2.py</code> <pre><code>def find_best_complete_products(self, max_cloud_cover: int | None = None, max_no_data_value: int = 5) -&gt; dict:\n    \"\"\"\n    Finds the best complete products for each Sentinel 2 tiles. This function will filter out all products that have\n    more than 5% of nodata values.\n\n    Filtered out tiles will be stored in `self.incomplete` and tiles for which\n    the search has found no results will be stored in `self.error_list`\n\n    Args:\n      max_cloud_cover: Max percentage of cloud cover allowed used for the search  (Default value = None)\n      max_no_data_value: Max percentage of no-data coverage by individual Sentinel 2 product  (Default value = 5)\n\n    Returns:\n        Dictionary of product IDs and their corresponding Sentinel 2 tile names.\n    \"\"\"\n    cloud_cover = self.max_cloud_cover\n    if max_cloud_cover:\n        cloud_cover = max_cloud_cover\n    no_data_value = self.max_no_data_value\n    if max_no_data_value:\n        no_data_value = max_no_data_value\n\n    tile_dict, incomplete_list, error_list = find_best_product_per_s2_tile(\n        date_ranges=self.date_ranges,\n        max_cloud_cover=cloud_cover,\n        s2_tile_grid_list=self.sentinel2_tile_list,\n        num_of_workers=4,\n        max_no_data_value=no_data_value,\n    )\n    self.successful_results = tile_dict\n    self.incomplete_results = incomplete_list\n    if incomplete_list:\n        self.logger.warning(\n            \"Warning, some of the input Sentinel 2 tiles do not have products covering the entire tile. \"\n            \"These tiles will need to be handled differently (ex. creating a mosaic with multiple products\"\n        )\n        self.logger.warning(f\"Incomplete list: {incomplete_list}\")\n    self.error_results = error_list\n    if error_list:\n        self.logger.warning(\n            \"Warning, products for some Sentinel 2 tiles could not be found. \"\n            \"Consider either extending date range input or max cloud cover\"\n        )\n        self.logger.warning(f\"Error list: {error_list}\")\n    return self.successful_results\n</code></pre>"},{"location":"reference/planetary_computer/sentinel_2/#planetary_computer.sentinel_2.BestProductsForFeatures.select_best_products_per_feature","title":"select_best_products_per_feature","text":"<pre><code>select_best_products_per_feature() -&gt; GeoDataFrame\n</code></pre> <p>Return a GeoDataFrame containing the best products for each Sentinel 2 tile.</p> Source code in <code>geospatial_tools/planetary_computer/sentinel_2.py</code> <pre><code>def select_best_products_per_feature(self) -&gt; GeoDataFrame:\n    \"\"\"Return a GeoDataFrame containing the best products for each Sentinel 2 tile.\"\"\"\n    spatial_join_results = spatial_join_within(\n        polygon_features=self.sentinel2_tiling_grid,\n        polygon_column=self.sentinel2_tiling_grid_column,\n        vector_features=self.vector_features,\n        vector_column_name=self.vector_features_column,\n    )\n    write_best_product_ids_to_dataframe(\n        spatial_join_results=spatial_join_results,\n        tile_dictionary=self.successful_results,\n        best_product_column=self.vector_features_best_product_column,\n        s2_tiles_column=self.vector_features_column,\n    )\n    self.vector_features_with_products = spatial_join_results\n    return self.vector_features_with_products\n</code></pre>"},{"location":"reference/planetary_computer/sentinel_2/#planetary_computer.sentinel_2.BestProductsForFeatures.to_file","title":"to_file","text":"<pre><code>to_file(output_dir: str | Path) -&gt; None\n</code></pre> <p>Parameters:</p> Name Type Description Default <code>output_dir</code> <code>str | Path</code> <p>Output directory used to write to file</p> required Source code in <code>geospatial_tools/planetary_computer/sentinel_2.py</code> <pre><code>def to_file(self, output_dir: str | pathlib.Path) -&gt; None:\n    \"\"\"\n\n    Args:\n      output_dir: Output directory used to write to file\n    \"\"\"\n    write_results_to_file(\n        cloud_cover=self.max_cloud_cover,\n        successful_results=self.successful_results,\n        incomplete_results=self.incomplete_results,\n        error_results=self.error_results,\n        output_dir=output_dir,\n    )\n</code></pre>"},{"location":"reference/planetary_computer/sentinel_2/#planetary_computer.sentinel_2.sentinel_2_complete_tile_search","title":"sentinel_2_complete_tile_search","text":"<pre><code>sentinel_2_complete_tile_search(\n    tile_id: int,\n    date_ranges: list[str],\n    max_cloud_cover: int,\n    max_no_data_value: int = 5,\n) -&gt; tuple[int, str, float | None, float | None] | None\n</code></pre> <p>Parameters:</p> Name Type Description Default <code>tile_id</code> <code>int</code> required <code>date_ranges</code> <code>list[str]</code> required <code>max_cloud_cover</code> <code>int</code> required <code>max_no_data_value</code> <code>int</code> <p>(Default value = 5)</p> <code>5</code> <p>Returns:</p> Source code in <code>geospatial_tools/planetary_computer/sentinel_2.py</code> <pre><code>def sentinel_2_complete_tile_search(\n    tile_id: int,\n    date_ranges: list[str],\n    max_cloud_cover: int,\n    max_no_data_value: int = 5,\n) -&gt; tuple[int, str, float | None, float | None] | None:\n    \"\"\"\n\n    Args:\n      tile_id:\n      date_ranges:\n      max_cloud_cover:\n      max_no_data_value: (Default value = 5)\n\n    Returns:\n\n\n    \"\"\"\n    client = StacSearch(PLANETARY_COMPUTER)\n    collection = \"sentinel-2-l2a\"\n    tile_ids = [tile_id]\n    query = {\"eo:cloud_cover\": {\"lt\": max_cloud_cover}, \"s2:mgrs_tile\": {\"in\": tile_ids}}\n    sortby = [{\"field\": \"properties.eo:cloud_cover\", \"direction\": \"asc\"}]\n\n    client.search_for_date_ranges(\n        date_ranges=date_ranges, collections=collection, query=query, sortby=sortby, limit=100\n    )\n    try:\n        sorted_items = client.sort_results_by_cloud_coverage()\n        if not sorted_items:\n            return tile_id, \"error: No results found\", None, None\n        filtered_items = client.filter_no_data(\n            property_name=\"s2:nodata_pixel_percentage\", max_no_data_value=max_no_data_value\n        )\n        if not filtered_items:\n            return tile_id, \"incomplete: No results found that cover the entire tile\", None, None\n        optimal_result = filtered_items[0]\n        if optimal_result:\n            return (\n                tile_id,\n                optimal_result.id,\n                optimal_result.properties[\"eo:cloud_cover\"],\n                optimal_result.properties[\"s2:nodata_pixel_percentage\"],\n            )\n\n    except (IndexError, TypeError) as error:\n        print(error)\n        return tile_id, f\"error: {error}\", None, None\n</code></pre>"},{"location":"reference/planetary_computer/sentinel_2/#planetary_computer.sentinel_2.find_best_product_per_s2_tile","title":"find_best_product_per_s2_tile","text":"<pre><code>find_best_product_per_s2_tile(\n    date_ranges: list[str],\n    max_cloud_cover: int,\n    s2_tile_grid_list: list,\n    max_no_data_value: int = 5,\n    num_of_workers: int = 4,\n)\n</code></pre> <p>Parameters:</p> Name Type Description Default <code>date_ranges</code> <code>list[str]</code> required <code>max_cloud_cover</code> <code>int</code> required <code>s2_tile_grid_list</code> <code>list</code> required <code>max_no_data_value</code> <code>int</code> <p>(Default value = 5)</p> <code>5</code> <code>num_of_workers</code> <code>int</code> <p>(Default value = 4)</p> <code>4</code> <p>Returns:</p> Source code in <code>geospatial_tools/planetary_computer/sentinel_2.py</code> <pre><code>def find_best_product_per_s2_tile(\n    date_ranges: list[str],\n    max_cloud_cover: int,\n    s2_tile_grid_list: list,\n    max_no_data_value: int = 5,\n    num_of_workers: int = 4,\n):\n    \"\"\"\n\n    Args:\n      date_ranges:\n      max_cloud_cover:\n      s2_tile_grid_list:\n      max_no_data_value:  (Default value = 5)\n      num_of_workers: (Default value = 4)\n\n    Returns:\n\n\n    \"\"\"\n    successful_results = {}\n    for tile in s2_tile_grid_list:\n        successful_results[tile] = \"\"\n    incomplete_results = []\n    error_results = []\n    with ThreadPoolExecutor(max_workers=num_of_workers) as executor:\n        future_to_tile = {\n            executor.submit(\n                sentinel_2_complete_tile_search,\n                tile_id=tile,\n                date_ranges=date_ranges,\n                max_cloud_cover=max_cloud_cover,\n                max_no_data_value=max_no_data_value,\n            ): tile\n            for tile in s2_tile_grid_list\n        }\n\n        for future in as_completed(future_to_tile):\n            tile_id, optimal_result_id, max_cloud_cover, no_data = future.result()\n            if optimal_result_id.startswith(\"error:\"):\n                error_results.append(tile_id)\n                continue\n            if optimal_result_id.startswith(\"incomplete:\"):\n                incomplete_results.append(tile_id)\n                continue\n            successful_results[tile_id] = {\"id\": optimal_result_id, \"cloud_cover\": max_cloud_cover, \"no_data\": no_data}\n        cleaned_successful_results = {k: v for k, v in successful_results.items() if v != \"\"}\n    return cleaned_successful_results, incomplete_results, error_results\n</code></pre>"},{"location":"reference/planetary_computer/sentinel_2/#planetary_computer.sentinel_2.write_best_product_ids_to_dataframe","title":"write_best_product_ids_to_dataframe","text":"<pre><code>write_best_product_ids_to_dataframe(\n    spatial_join_results: GeoDataFrame,\n    tile_dictionary: dict,\n    best_product_column: str = \"best_s2_product_id\",\n    s2_tiles_column: str = \"s2_tiles\",\n    logger: Logger = LOGGER,\n)\n</code></pre> <p>Parameters:</p> Name Type Description Default <code>spatial_join_results</code> <code>GeoDataFrame</code> required <code>tile_dictionary</code> <code>dict</code> required <code>best_product_column</code> <code>str</code> <code>'best_s2_product_id'</code> <code>s2_tiles_column</code> <code>str</code> <code>'s2_tiles'</code> <code>logger</code> <code>Logger</code> <code>LOGGER</code> <p>Returns:</p> Source code in <code>geospatial_tools/planetary_computer/sentinel_2.py</code> <pre><code>def write_best_product_ids_to_dataframe(\n    spatial_join_results: GeoDataFrame,\n    tile_dictionary: dict,\n    best_product_column: str = \"best_s2_product_id\",\n    s2_tiles_column: str = \"s2_tiles\",\n    logger: logging.Logger = LOGGER,\n):\n    \"\"\"\n\n    Args:\n      spatial_join_results:\n      tile_dictionary:\n      best_product_column:\n      s2_tiles_column:\n      logger:\n\n    Returns:\n\n\n    \"\"\"\n    logger.info(\"Writing best product IDs to dataframe\")\n    spatial_join_results[best_product_column] = spatial_join_results[s2_tiles_column].apply(\n        lambda x: _get_best_product_id_for_each_grid_tile(s2_tile_search_results=tile_dictionary, feature_s2_tiles=x)\n    )\n</code></pre>"},{"location":"reference/planetary_computer/sentinel_2/#planetary_computer.sentinel_2.write_results_to_file","title":"write_results_to_file","text":"<pre><code>write_results_to_file(\n    cloud_cover: int,\n    successful_results: dict,\n    incomplete_results: list | None = None,\n    error_results: list | None = None,\n    output_dir: str | Path = DATA_DIR,\n    logger: Logger = LOGGER,\n) -&gt; dict\n</code></pre> <p>Parameters:</p> Name Type Description Default <code>cloud_cover</code> <code>int</code> required <code>successful_results</code> <code>dict</code> required <code>incomplete_results</code> <code>list | None</code> <code>None</code> <code>error_results</code> <code>list | None</code> <code>None</code> <code>output_dir</code> <code>str | Path</code> <code>DATA_DIR</code> <code>logger</code> <code>Logger</code> <code>LOGGER</code> <p>Returns:</p> Source code in <code>geospatial_tools/planetary_computer/sentinel_2.py</code> <pre><code>def write_results_to_file(\n    cloud_cover: int,\n    successful_results: dict,\n    incomplete_results: list | None = None,\n    error_results: list | None = None,\n    output_dir: str | pathlib.Path = DATA_DIR,\n    logger: logging.Logger = LOGGER,\n) -&gt; dict:\n    \"\"\"\n\n    Args:\n      cloud_cover:\n      successful_results:\n      incomplete_results:\n      error_results:\n      output_dir:\n      logger:\n\n    Returns:\n\n\n    \"\"\"\n    tile_filename = output_dir / f\"data_lt{cloud_cover}cc.json\"\n    with open(tile_filename, \"w\", encoding=\"utf-8\") as json_file:\n        json.dump(successful_results, json_file, indent=4)\n    logger.info(f\"Results have been written to {tile_filename}\")\n\n    incomplete_filename = \"None\"\n    if incomplete_results:\n        incomplete_dict = {\"incomplete\": incomplete_results}\n        incomplete_filename = output_dir / f\"incomplete_lt{cloud_cover}cc.json\"\n        with open(incomplete_filename, \"w\", encoding=\"utf-8\") as json_file:\n            json.dump(incomplete_dict, json_file, indent=4)\n        logger.info(f\"Incomplete results have been written to {incomplete_filename}\")\n\n    error_filename = \"None\"\n    if error_results:\n        error_dict = {\"errors\": error_results}\n        error_filename = output_dir / f\"errors_lt{cloud_cover}cc.json\"\n        with open(error_filename, \"w\", encoding=\"utf-8\") as json_file:\n            json.dump(error_dict, json_file, indent=4)\n        logger.info(f\"Errors results have been written to {error_filename}\")\n\n    return {\n        \"tile_filename\": tile_filename,\n        \"incomplete_filename\": incomplete_filename,\n        \"errors_filename\": error_filename,\n    }\n</code></pre>"},{"location":"reference/planetary_computer/sentinel_2/#planetary_computer.sentinel_2.download_and_process_sentinel2_asset","title":"download_and_process_sentinel2_asset","text":"<pre><code>download_and_process_sentinel2_asset(\n    product_id: str,\n    product_bands: list[str],\n    collections: str = \"sentinel-2-l2a\",\n    target_projection: int | str | None = None,\n    base_directory: str | Path = DATA_DIR,\n    delete_intermediate_files: bool = False,\n    logger: Logger = LOGGER,\n) -&gt; Asset\n</code></pre> <p>This function downloads a Sentinel 2 product based on the product ID provided.</p> <p>It will download the individual asset bands provided in the <code>bands</code> argument, merge then all in a single tif and then reproject them to the input CRS.</p> <p>Parameters:</p> Name Type Description Default <code>product_id</code> <code>str</code> <p>ID of the Sentinel 2 product to be downloaded</p> required <code>product_bands</code> <code>list[str]</code> <p>List of the product bands to be downloaded</p> required <code>collections</code> <code>str</code> <p>Collections to be downloaded from. Defaults to <code>sentinel-2-l2a</code></p> <code>'sentinel-2-l2a'</code> <code>target_projection</code> <code>int | str | None</code> <p>The CRS project for the end product. If <code>None</code>, the reprojection step will be skipped</p> <code>None</code> <code>stac_client</code> <p>StacSearch client to used. A new one will be created if not provided</p> required <code>base_directory</code> <code>str | Path</code> <p>The base directory path where the downloaded files will be stored</p> <code>DATA_DIR</code> <code>delete_intermediate_files</code> <code>bool</code> <p>Flag to determine if intermediate files should be deleted. Defaults to False</p> <code>False</code> <code>logger</code> <code>Logger</code> <p>Logger instance</p> <code>LOGGER</code> <p>Returns:</p> Type Description <code>Asset</code> <p>Asset instance</p> Source code in <code>geospatial_tools/planetary_computer/sentinel_2.py</code> <pre><code>def download_and_process_sentinel2_asset(\n    product_id: str,\n    product_bands: list[str],\n    collections: str = \"sentinel-2-l2a\",\n    target_projection: int | str | None = None,\n    base_directory: str | pathlib.Path = DATA_DIR,\n    delete_intermediate_files: bool = False,\n    logger: logging.Logger = LOGGER,\n) -&gt; Asset:\n    \"\"\"\n    This function downloads a Sentinel 2 product based on the product ID provided.\n\n    It will download the individual asset bands provided in the `bands` argument,\n    merge then all in a single tif and then reproject them to the input CRS.\n\n    Args:\n      product_id: ID of the Sentinel 2 product to be downloaded\n      product_bands: List of the product bands to be downloaded\n      collections: Collections to be downloaded from. Defaults to `sentinel-2-l2a`\n      target_projection: The CRS project for the end product. If `None`, the reprojection step will be\n        skipped\n      stac_client: StacSearch client to used. A new one will be created if not provided\n      base_directory: The base directory path where the downloaded files will be stored\n      delete_intermediate_files: Flag to determine if intermediate files should be deleted. Defaults to False\n      logger: Logger instance\n\n    Returns:\n        Asset instance\n    \"\"\"\n    base_file_name = f\"{base_directory}/{product_id}\"\n    merged_file = f\"{base_file_name}_merged.tif\"\n    reprojected_file = f\"{base_file_name}_reprojected.tif\"\n\n    merged_file_exists = pathlib.Path(merged_file).exists()\n    reprojected_file_exists = pathlib.Path(reprojected_file).exists()\n\n    if reprojected_file_exists:\n        logger.info(f\"Reprojected file [{reprojected_file}] already exists\")\n        asset = Asset(asset_id=product_id, bands=product_bands, reprojected_asset=reprojected_file)\n        return asset\n\n    if merged_file_exists:\n        logger.info(f\"Merged file [{merged_file}] already exists\")\n        asset = Asset(asset_id=product_id, bands=product_bands, merged_asset_path=merged_file)\n        if target_projection:\n            logger.info(f\"Reprojecting merged file [{merged_file}]\")\n            asset.reproject_merged_asset(\n                base_directory=base_directory,\n                target_projection=target_projection,\n                delete_merged_asset=delete_intermediate_files,\n            )\n        return asset\n\n    stac_client = StacSearch(catalog_name=PLANETARY_COMPUTER)\n    items = stac_client.search(collections=collections, ids=[product_id])\n    logger.info(items)\n    asset_list = stac_client.download_search_results(bands=product_bands, base_directory=base_directory)\n    logger.info(asset_list)\n    asset = asset_list[0]\n    asset.merge_asset(base_directory=base_directory, delete_sub_items=delete_intermediate_files)\n    if not target_projection:\n        logger.info(\"Skipping reprojection\")\n        return asset\n    if target_projection:\n        asset.reproject_merged_asset(\n            target_projection=target_projection,\n            base_directory=base_directory,\n            delete_merged_asset=delete_intermediate_files,\n        )\n    return asset\n</code></pre>"},{"location":"reference/radar/","title":"radar","text":""},{"location":"reference/radar/#radar.nimrod","title":"nimrod","text":""},{"location":"reference/radar/#radar.nimrod.extract_nimrod_from_archive","title":"extract_nimrod_from_archive","text":"<pre><code>extract_nimrod_from_archive(\n    archive_file_path: str | Path, output_directory: str | Path = None\n) -&gt; Path\n</code></pre> <p>Extract nimrod data from an archive file. If no output directory is provided, the extracted data will be saved to the archive file's directory.</p> <p>Parameters:</p> Name Type Description Default <code>archive_file_path</code> <code>str | Path</code> <p>Path to the archive file</p> required <code>output_directory</code> <code>str | Path</code> <p>Optional output directory.</p> <code>None</code> <p>Returns:</p> Type Description <code>Path</code> <p>Path to the extracted nimrod data file</p> Source code in <code>geospatial_tools/radar/nimrod.py</code> <pre><code>def extract_nimrod_from_archive(archive_file_path: str | Path, output_directory: str | Path = None) -&gt; Path:\n    \"\"\"\n    Extract nimrod data from an archive file. If no output directory is provided, the extracted data will be saved to\n    the archive file's directory.\n\n    Args:\n        archive_file_path: Path to the archive file\n        output_directory: Optional output directory.\n\n    Returns:\n            Path to the extracted nimrod data file\n    \"\"\"\n    if isinstance(archive_file_path, str):\n        archive_file_path = Path(archive_file_path)\n    full_path = archive_file_path.resolve()\n    parent_folder = archive_file_path.parent\n    filename = archive_file_path.stem\n\n    target_folder = None\n    if output_directory:\n        if isinstance(output_directory, str):\n            output_directory = Path(output_directory)\n        target_folder = output_directory\n\n    if not target_folder:\n        target_folder = parent_folder / filename\n\n    target_folder.mkdir(parents=True, exist_ok=True)\n    gzip_file_headers = parse_gzip_header(archive_file_path)\n    contained_filename = gzip_file_headers[\"original_name\"]\n\n    with gzip.open(full_path, \"rb\") as f_in:\n        if not contained_filename:\n            contained_filename = filename\n        print(f\"Filename {contained_filename}\")\n        out_path = target_folder / contained_filename\n\n        with open(out_path, \"wb\") as f_out:\n            shutil.copyfileobj(f_in, f_out)\n\n    return out_path\n</code></pre>"},{"location":"reference/radar/#radar.nimrod.load_nimrod_cubes","title":"load_nimrod_cubes","text":"<pre><code>load_nimrod_cubes(filenames: list[str | Path]) -&gt; Generator[Cube | Any, Any, None]\n</code></pre> <p>Parameters:</p> Name Type Description Default <code>filenames</code> <code>list[str | Path]</code> <p>List of nimrod files</p> required <p>Returns:</p> Type Description <code>None</code> <p>Generator of cubes</p> Source code in <code>geospatial_tools/radar/nimrod.py</code> <pre><code>def load_nimrod_cubes(filenames: list[str | Path]) -&gt; Generator[Cube | Any, Any, None]:\n    \"\"\"\n\n    Args:\n        filenames: List of nimrod files\n\n    Returns:\n        Generator of cubes\n\n    \"\"\"\n    # Ensure filenames are strings, as iris load_cubes might expect strings\n    filenames = [str(f) for f in filenames]\n    cubes = load_cubes(filenames)\n    return cubes\n</code></pre>"},{"location":"reference/radar/#radar.nimrod.load_nimrod_from_archive","title":"load_nimrod_from_archive","text":"<pre><code>load_nimrod_from_archive(filename: str | Path) -&gt; Generator[Cube | Any, Any, None]\n</code></pre> <p>Parameters:</p> Name Type Description Default <code>filename</code> <code>str | Path</code> <p>Path to the archive file</p> required <p>Returns:</p> Type Description <code>None</code> <p>Generator of cubes</p> Source code in <code>geospatial_tools/radar/nimrod.py</code> <pre><code>def load_nimrod_from_archive(filename: str | Path) -&gt; Generator[Cube | Any, Any, None]:\n    \"\"\"\n\n    Args:\n        filename: Path to the archive file\n\n    Returns:\n        Generator of cubes\n\n    \"\"\"\n    nimrod_extracted_file = extract_nimrod_from_archive(filename)\n    # The extraction returns a single file path. We load that file.\n    cubes = load_nimrod_cubes([nimrod_extracted_file])\n    return cubes\n</code></pre>"},{"location":"reference/radar/#radar.nimrod.merge_nimrod_cubes","title":"merge_nimrod_cubes","text":"<pre><code>merge_nimrod_cubes(cubes: list[Cube]) -&gt; Cube\n</code></pre> <p>Parameters:</p> Name Type Description Default <code>cubes</code> <code>list[Cube]</code> <p>List of cubes to merge</p> required <p>Returns:</p> Type Description <code>Cube</code> <p>Merged cube</p> Source code in <code>geospatial_tools/radar/nimrod.py</code> <pre><code>def merge_nimrod_cubes(cubes: list[Cube]) -&gt; Cube:\n    \"\"\"\n\n    Args:\n        cubes: List of cubes to merge\n\n    Returns:\n        Merged cube\n    \"\"\"\n    cubes = CubeList(cubes)\n    merged_cubes = cubes.merge_cube()\n    return merged_cubes\n</code></pre>"},{"location":"reference/radar/#radar.nimrod.mean_nimrod_cubes","title":"mean_nimrod_cubes","text":"<pre><code>mean_nimrod_cubes(merged_cubes: Cube) -&gt; Cube\n</code></pre> <p>Parameters:</p> Name Type Description Default <code>merged_cubes</code> <code>Cube</code> <p>Merged cube</p> required <p>Returns:</p> Type Description <code>Cube</code> <p>Mean cube</p> Source code in <code>geospatial_tools/radar/nimrod.py</code> <pre><code>def mean_nimrod_cubes(merged_cubes: Cube) -&gt; Cube:\n    \"\"\"\n\n    Args:\n        merged_cubes: Merged cube\n\n    Returns:\n        Mean cube\n    \"\"\"\n    mean_cube = merged_cubes.collapsed(\"time\", MEAN)\n    return mean_cube\n</code></pre>"},{"location":"reference/radar/#radar.nimrod.write_cube_to_file","title":"write_cube_to_file","text":"<pre><code>write_cube_to_file(cube: Cube, output_name: str | Path) -&gt; None\n</code></pre> <p>Save a nimrod cube to a Netcdf file.</p> <p>Parameters:</p> Name Type Description Default <code>cube</code> <code>Cube</code> <p>Cube to save</p> required <code>output_name</code> <code>str | Path</code> <p>Output filename</p> required Source code in <code>geospatial_tools/radar/nimrod.py</code> <pre><code>def write_cube_to_file(cube: Cube, output_name: str | Path) -&gt; None:\n    \"\"\"\n    Save a nimrod cube to a Netcdf file.\n\n    Args:\n        cube: Cube to save\n        output_name: Output filename\n    \"\"\"\n    netcdf.save(cube, output_name)\n</code></pre>"},{"location":"reference/radar/#radar.nimrod.assert_dataset_time_dim_is_valid","title":"assert_dataset_time_dim_is_valid","text":"<pre><code>assert_dataset_time_dim_is_valid(\n    dataset: Dataset, time_dimension_name: str = \"time\"\n) -&gt; None\n</code></pre> Ths function checks that the time dimension of a given dataset <ul> <li>Is composed of 5-minute time bins - Which is the native Nimrod format</li> <li>Contains a continuous time series, without any holes - which would lead to false statistics when resampling</li> </ul> <p>Parameters:</p> Name Type Description Default <code>dataset</code> <code>Dataset</code> <p>Merged nimrod cube</p> required <code>time_dimension_name</code> <code>str</code> <p>Name of the time dimension</p> <code>'time'</code> <p>Returns:</p> Type Description <code>None</code> <p>Bool value indicating if the time bins are 5 minutes long and if there are no</p> <code>None</code> <p>gaps in the time series</p> Source code in <code>geospatial_tools/radar/nimrod.py</code> <pre><code>def assert_dataset_time_dim_is_valid(dataset: xr.Dataset, time_dimension_name: str = \"time\") -&gt; None:\n    \"\"\"\n    Ths function checks that the time dimension of a given dataset :\n        - Is composed of 5-minute time bins - Which is the native Nimrod format\n        - Contains a continuous time series, without any holes - which would lead to false statistics when resampling\n\n    Args:\n        dataset: Merged nimrod cube\n        time_dimension_name: Name of the time dimension\n\n    Returns:\n        Bool value indicating if the time bins are 5 minutes long and if there are no\n        gaps in the time series\n    \"\"\"\n    dataset_time_dimension = dataset[time_dimension_name]\n    if not dataset_time_dimension.to_index().is_monotonic_increasing:\n        raise AssertionError(\"Time is not sorted ascending\")\n    if not dataset_time_dimension.to_index().is_unique:\n        duplicates = dataset_time_dimension.to_index()[dataset_time_dimension.to_index().duplicated(keep=False)]\n        raise AssertionError(f\"Duplicate timestamps present: {duplicates[:10]} ...\")\n\n    difference_between_timesteps = dataset_time_dimension.diff(time_dimension_name)\n    if (difference_between_timesteps != FIVE_MIN).any():\n        larger_time_gaps = np.nonzero((difference_between_timesteps != FIVE_MIN).compute().to_numpy())[0][:5]\n        raise AssertionError(\n            f\"Non-5min gaps at positions {larger_time_gaps} \"\n            f\"(examples: {difference_between_timesteps.isel({time_dimension_name: larger_time_gaps}).to_numpy()})\"\n        )\n\n    start = pd.Timestamp(dataset_time_dimension.to_numpy()[0])\n    end = pd.Timestamp(dataset_time_dimension.to_numpy()[-1])\n    expected_index = pd.date_range(start=start, end=end, freq=\"5min\", inclusive=\"both\")\n    dataset_index = dataset_time_dimension.to_index()\n    missing_indexes = expected_index.difference(dataset_index)\n    if len(missing_indexes) &gt; 0:\n        raise AssertionError(f\"missing {len(missing_indexes)} stamps; first few: {missing_indexes[:10]}\")\n</code></pre>"},{"location":"reference/radar/#radar.nimrod.resample_nimrod_timebox_30min_bins","title":"resample_nimrod_timebox_30min_bins","text":"<pre><code>resample_nimrod_timebox_30min_bins(\n    filenames: list[str | Path], output_name: str | Path\n) -&gt; str | Path\n</code></pre> <p>This will resample nimrod data's bins to 30-minute interval instead of their normal 5-minute interval. It uses a mean resampling, and creates time bins like follows :</p> <p>ex. [[09h00, &lt; 9h05], [09h05, &lt; 9h10], ... ] -&gt; [[09h00, &lt; 9h30], [09h30, &lt; 10h], ... ]</p> <p>Parameters:</p> Name Type Description Default <code>filenames</code> <code>list[str | Path]</code> <p>List of netcdf nimrod files</p> required <code>output_name</code> <code>str | Path</code> <p>Output filename</p> required <p>Returns:</p> Type Description <code>str | Path</code> <p>Path to the output file</p> Source code in <code>geospatial_tools/radar/nimrod.py</code> <pre><code>def resample_nimrod_timebox_30min_bins(filenames: list[str | Path], output_name: str | Path) -&gt; str | Path:\n    \"\"\"\n    This will resample nimrod data's bins to 30-minute interval instead of their\n    normal 5-minute interval. It uses a mean resampling, and creates time bins like\n    follows :\n\n    ex. [[09h00, &lt; 9h05], [09h05, &lt; 9h10], ... ] -&gt; [[09h00, &lt; 9h30], [09h30, &lt; 10h], ... ]\n\n    Args:\n        filenames: List of netcdf nimrod files\n        output_name: Output filename\n\n    Returns:\n        Path to the output file\n    \"\"\"\n    ds = xr.open_mfdataset(filenames, combine=\"nested\", concat_dim=\"time\")\n    ds_30min = ds.resample(time=\"30min\").mean()\n    ds_30min.to_netcdf(output_name)\n    return output_name\n</code></pre>"},{"location":"reference/radar/nimrod/","title":"nimrod","text":""},{"location":"reference/radar/nimrod/#radar.nimrod.extract_nimrod_from_archive","title":"extract_nimrod_from_archive","text":"<pre><code>extract_nimrod_from_archive(\n    archive_file_path: str | Path, output_directory: str | Path = None\n) -&gt; Path\n</code></pre> <p>Extract nimrod data from an archive file. If no output directory is provided, the extracted data will be saved to the archive file's directory.</p> <p>Parameters:</p> Name Type Description Default <code>archive_file_path</code> <code>str | Path</code> <p>Path to the archive file</p> required <code>output_directory</code> <code>str | Path</code> <p>Optional output directory.</p> <code>None</code> <p>Returns:</p> Type Description <code>Path</code> <p>Path to the extracted nimrod data file</p> Source code in <code>geospatial_tools/radar/nimrod.py</code> <pre><code>def extract_nimrod_from_archive(archive_file_path: str | Path, output_directory: str | Path = None) -&gt; Path:\n    \"\"\"\n    Extract nimrod data from an archive file. If no output directory is provided, the extracted data will be saved to\n    the archive file's directory.\n\n    Args:\n        archive_file_path: Path to the archive file\n        output_directory: Optional output directory.\n\n    Returns:\n            Path to the extracted nimrod data file\n    \"\"\"\n    if isinstance(archive_file_path, str):\n        archive_file_path = Path(archive_file_path)\n    full_path = archive_file_path.resolve()\n    parent_folder = archive_file_path.parent\n    filename = archive_file_path.stem\n\n    target_folder = None\n    if output_directory:\n        if isinstance(output_directory, str):\n            output_directory = Path(output_directory)\n        target_folder = output_directory\n\n    if not target_folder:\n        target_folder = parent_folder / filename\n\n    target_folder.mkdir(parents=True, exist_ok=True)\n    gzip_file_headers = parse_gzip_header(archive_file_path)\n    contained_filename = gzip_file_headers[\"original_name\"]\n\n    with gzip.open(full_path, \"rb\") as f_in:\n        if not contained_filename:\n            contained_filename = filename\n        print(f\"Filename {contained_filename}\")\n        out_path = target_folder / contained_filename\n\n        with open(out_path, \"wb\") as f_out:\n            shutil.copyfileobj(f_in, f_out)\n\n    return out_path\n</code></pre>"},{"location":"reference/radar/nimrod/#radar.nimrod.load_nimrod_cubes","title":"load_nimrod_cubes","text":"<pre><code>load_nimrod_cubes(filenames: list[str | Path]) -&gt; Generator[Cube | Any, Any, None]\n</code></pre> <p>Parameters:</p> Name Type Description Default <code>filenames</code> <code>list[str | Path]</code> <p>List of nimrod files</p> required <p>Returns:</p> Type Description <code>None</code> <p>Generator of cubes</p> Source code in <code>geospatial_tools/radar/nimrod.py</code> <pre><code>def load_nimrod_cubes(filenames: list[str | Path]) -&gt; Generator[Cube | Any, Any, None]:\n    \"\"\"\n\n    Args:\n        filenames: List of nimrod files\n\n    Returns:\n        Generator of cubes\n\n    \"\"\"\n    # Ensure filenames are strings, as iris load_cubes might expect strings\n    filenames = [str(f) for f in filenames]\n    cubes = load_cubes(filenames)\n    return cubes\n</code></pre>"},{"location":"reference/radar/nimrod/#radar.nimrod.load_nimrod_from_archive","title":"load_nimrod_from_archive","text":"<pre><code>load_nimrod_from_archive(filename: str | Path) -&gt; Generator[Cube | Any, Any, None]\n</code></pre> <p>Parameters:</p> Name Type Description Default <code>filename</code> <code>str | Path</code> <p>Path to the archive file</p> required <p>Returns:</p> Type Description <code>None</code> <p>Generator of cubes</p> Source code in <code>geospatial_tools/radar/nimrod.py</code> <pre><code>def load_nimrod_from_archive(filename: str | Path) -&gt; Generator[Cube | Any, Any, None]:\n    \"\"\"\n\n    Args:\n        filename: Path to the archive file\n\n    Returns:\n        Generator of cubes\n\n    \"\"\"\n    nimrod_extracted_file = extract_nimrod_from_archive(filename)\n    # The extraction returns a single file path. We load that file.\n    cubes = load_nimrod_cubes([nimrod_extracted_file])\n    return cubes\n</code></pre>"},{"location":"reference/radar/nimrod/#radar.nimrod.merge_nimrod_cubes","title":"merge_nimrod_cubes","text":"<pre><code>merge_nimrod_cubes(cubes: list[Cube]) -&gt; Cube\n</code></pre> <p>Parameters:</p> Name Type Description Default <code>cubes</code> <code>list[Cube]</code> <p>List of cubes to merge</p> required <p>Returns:</p> Type Description <code>Cube</code> <p>Merged cube</p> Source code in <code>geospatial_tools/radar/nimrod.py</code> <pre><code>def merge_nimrod_cubes(cubes: list[Cube]) -&gt; Cube:\n    \"\"\"\n\n    Args:\n        cubes: List of cubes to merge\n\n    Returns:\n        Merged cube\n    \"\"\"\n    cubes = CubeList(cubes)\n    merged_cubes = cubes.merge_cube()\n    return merged_cubes\n</code></pre>"},{"location":"reference/radar/nimrod/#radar.nimrod.mean_nimrod_cubes","title":"mean_nimrod_cubes","text":"<pre><code>mean_nimrod_cubes(merged_cubes: Cube) -&gt; Cube\n</code></pre> <p>Parameters:</p> Name Type Description Default <code>merged_cubes</code> <code>Cube</code> <p>Merged cube</p> required <p>Returns:</p> Type Description <code>Cube</code> <p>Mean cube</p> Source code in <code>geospatial_tools/radar/nimrod.py</code> <pre><code>def mean_nimrod_cubes(merged_cubes: Cube) -&gt; Cube:\n    \"\"\"\n\n    Args:\n        merged_cubes: Merged cube\n\n    Returns:\n        Mean cube\n    \"\"\"\n    mean_cube = merged_cubes.collapsed(\"time\", MEAN)\n    return mean_cube\n</code></pre>"},{"location":"reference/radar/nimrod/#radar.nimrod.write_cube_to_file","title":"write_cube_to_file","text":"<pre><code>write_cube_to_file(cube: Cube, output_name: str | Path) -&gt; None\n</code></pre> <p>Save a nimrod cube to a Netcdf file.</p> <p>Parameters:</p> Name Type Description Default <code>cube</code> <code>Cube</code> <p>Cube to save</p> required <code>output_name</code> <code>str | Path</code> <p>Output filename</p> required Source code in <code>geospatial_tools/radar/nimrod.py</code> <pre><code>def write_cube_to_file(cube: Cube, output_name: str | Path) -&gt; None:\n    \"\"\"\n    Save a nimrod cube to a Netcdf file.\n\n    Args:\n        cube: Cube to save\n        output_name: Output filename\n    \"\"\"\n    netcdf.save(cube, output_name)\n</code></pre>"},{"location":"reference/radar/nimrod/#radar.nimrod.assert_dataset_time_dim_is_valid","title":"assert_dataset_time_dim_is_valid","text":"<pre><code>assert_dataset_time_dim_is_valid(\n    dataset: Dataset, time_dimension_name: str = \"time\"\n) -&gt; None\n</code></pre> Ths function checks that the time dimension of a given dataset <ul> <li>Is composed of 5-minute time bins - Which is the native Nimrod format</li> <li>Contains a continuous time series, without any holes - which would lead to false statistics when resampling</li> </ul> <p>Parameters:</p> Name Type Description Default <code>dataset</code> <code>Dataset</code> <p>Merged nimrod cube</p> required <code>time_dimension_name</code> <code>str</code> <p>Name of the time dimension</p> <code>'time'</code> <p>Returns:</p> Type Description <code>None</code> <p>Bool value indicating if the time bins are 5 minutes long and if there are no</p> <code>None</code> <p>gaps in the time series</p> Source code in <code>geospatial_tools/radar/nimrod.py</code> <pre><code>def assert_dataset_time_dim_is_valid(dataset: xr.Dataset, time_dimension_name: str = \"time\") -&gt; None:\n    \"\"\"\n    Ths function checks that the time dimension of a given dataset :\n        - Is composed of 5-minute time bins - Which is the native Nimrod format\n        - Contains a continuous time series, without any holes - which would lead to false statistics when resampling\n\n    Args:\n        dataset: Merged nimrod cube\n        time_dimension_name: Name of the time dimension\n\n    Returns:\n        Bool value indicating if the time bins are 5 minutes long and if there are no\n        gaps in the time series\n    \"\"\"\n    dataset_time_dimension = dataset[time_dimension_name]\n    if not dataset_time_dimension.to_index().is_monotonic_increasing:\n        raise AssertionError(\"Time is not sorted ascending\")\n    if not dataset_time_dimension.to_index().is_unique:\n        duplicates = dataset_time_dimension.to_index()[dataset_time_dimension.to_index().duplicated(keep=False)]\n        raise AssertionError(f\"Duplicate timestamps present: {duplicates[:10]} ...\")\n\n    difference_between_timesteps = dataset_time_dimension.diff(time_dimension_name)\n    if (difference_between_timesteps != FIVE_MIN).any():\n        larger_time_gaps = np.nonzero((difference_between_timesteps != FIVE_MIN).compute().to_numpy())[0][:5]\n        raise AssertionError(\n            f\"Non-5min gaps at positions {larger_time_gaps} \"\n            f\"(examples: {difference_between_timesteps.isel({time_dimension_name: larger_time_gaps}).to_numpy()})\"\n        )\n\n    start = pd.Timestamp(dataset_time_dimension.to_numpy()[0])\n    end = pd.Timestamp(dataset_time_dimension.to_numpy()[-1])\n    expected_index = pd.date_range(start=start, end=end, freq=\"5min\", inclusive=\"both\")\n    dataset_index = dataset_time_dimension.to_index()\n    missing_indexes = expected_index.difference(dataset_index)\n    if len(missing_indexes) &gt; 0:\n        raise AssertionError(f\"missing {len(missing_indexes)} stamps; first few: {missing_indexes[:10]}\")\n</code></pre>"},{"location":"reference/radar/nimrod/#radar.nimrod.resample_nimrod_timebox_30min_bins","title":"resample_nimrod_timebox_30min_bins","text":"<pre><code>resample_nimrod_timebox_30min_bins(\n    filenames: list[str | Path], output_name: str | Path\n) -&gt; str | Path\n</code></pre> <p>This will resample nimrod data's bins to 30-minute interval instead of their normal 5-minute interval. It uses a mean resampling, and creates time bins like follows :</p> <p>ex. [[09h00, &lt; 9h05], [09h05, &lt; 9h10], ... ] -&gt; [[09h00, &lt; 9h30], [09h30, &lt; 10h], ... ]</p> <p>Parameters:</p> Name Type Description Default <code>filenames</code> <code>list[str | Path]</code> <p>List of netcdf nimrod files</p> required <code>output_name</code> <code>str | Path</code> <p>Output filename</p> required <p>Returns:</p> Type Description <code>str | Path</code> <p>Path to the output file</p> Source code in <code>geospatial_tools/radar/nimrod.py</code> <pre><code>def resample_nimrod_timebox_30min_bins(filenames: list[str | Path], output_name: str | Path) -&gt; str | Path:\n    \"\"\"\n    This will resample nimrod data's bins to 30-minute interval instead of their\n    normal 5-minute interval. It uses a mean resampling, and creates time bins like\n    follows :\n\n    ex. [[09h00, &lt; 9h05], [09h05, &lt; 9h10], ... ] -&gt; [[09h00, &lt; 9h30], [09h30, &lt; 10h], ... ]\n\n    Args:\n        filenames: List of netcdf nimrod files\n        output_name: Output filename\n\n    Returns:\n        Path to the output file\n    \"\"\"\n    ds = xr.open_mfdataset(filenames, combine=\"nested\", concat_dim=\"time\")\n    ds_30min = ds.resample(time=\"30min\").mean()\n    ds_30min.to_netcdf(output_name)\n    return output_name\n</code></pre>"},{"location":"sections/dev-guide/developer-guide/","title":"Dev Guide","text":""},{"location":"sections/user-guide/notebook-examples/","title":"Notebook Examples","text":"<p>There are a few notebook examples available.</p> <ul> <li>How to use STAC API</li> <li>Exploring Sentinel 2 data from Planetary Computer</li> </ul>"}]}